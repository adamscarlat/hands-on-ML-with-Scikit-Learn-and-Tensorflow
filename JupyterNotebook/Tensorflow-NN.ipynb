{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network with TensorFlow\n",
    "\n",
    "Using TensorFlow's API to build a neural network as a tensor graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf#Reset the tf graph\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset the TF graph\n",
    "\n",
    "def reset_graph(seed=42): \n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Construction phase\n",
    "----------------------------------------\n",
    "Here we define the network in terms of: \n",
    "- input, hidden, output layers\n",
    "- activation function: ReLu\n",
    "- loss function: cross-entropy\n",
    "- optimizer: mini-batch GD\n",
    "- performacne evaluation: accuracy\n",
    "'''\n",
    "\n",
    "#specify the NN architecture\n",
    "\n",
    "n_inputs = 28*28 # MNIST \n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We use placeholders for the input/output of the network. \n",
    "#For X, we know the number of features each instance will have (784) but we don't know\n",
    "#how many instances each batch of the mini-batch GD will contain.\n",
    "\n",
    "#Same for y, we know it's a single label per training instance but still rely on the\n",
    "#number of instances in our mini-batch. Each Yi will contain the error of example Xi\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function creates a layer in the NN. It takes an input layer X (units connected to this layer),\n",
    "#number of neurons (units in the layer), name and an activation function.\n",
    "\n",
    "#This has a TF implementation that does it- see TF's fully_connected function\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None): \n",
    "    #add namescope for better visualization in tensorboard\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        #get number of features from input\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        \n",
    "        #initialize W matrix with random weight.\n",
    "        #W is the shape (n_features_input, n_neurons_nextlayer)\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) \n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        \n",
    "        #bias edges in hidden layer\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        \n",
    "        #hidden layer- net\n",
    "        z = tf.matmul(X, W) + b\n",
    "        \n",
    "        #hidden layer- out\n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z) \n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the DNN:\n",
    "    #input layer: a training instance\n",
    "    #hidden layer 1: 300\n",
    "    #hidden layer 2: 100\n",
    "    #output layer: 10\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\") \n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\") \n",
    "    logits = neuron_layer(hidden2, n_outputs, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "#The TF implementation for our 'neuron_layer' function\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\", reuse=True) \n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\", reuse=True) \n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\",\n",
    "                                 activation_fn=None, reuse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define cross-entropy loss function for the output layer.\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    \n",
    "    #computes the cross-entropy loss of each output node\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    \n",
    "    #computes the total error as a mean over the cross-entropies\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define a gradient-descent optimizer to minimize the loss function\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    #this will minimize the loss, a TF node that contains the rest \n",
    "    #of our nodes: X,W and y\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) \n",
    "    training_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    \n",
    "    #foreach prediction i (logits) determine wether the highest logit \n",
    "    #(amonge the topk predictions where k=1) corresponds to the target class\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializer and saver\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Execution Phase\n",
    "-------------------\n",
    "- Load, scale and split the MNIST data: done by TF read_data_sets method\n",
    "- Define execution parameters: ephocs and batch size\n",
    "- Run training, evaluate every epoch\n",
    "- Save network to disk\n",
    "'''\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#load data using TF's API. Loads, scales and shuffles the data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "#define mini-batch GD parameters\n",
    "n_epochs = 30\n",
    "batch_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.94 Test accuracy: 0.9013\n",
      "1 Train accuracy: 0.9 Test accuracy: 0.9224\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9337\n",
      "3 Train accuracy: 0.96 Test accuracy: 0.9376\n",
      "4 Train accuracy: 0.9 Test accuracy: 0.9428\n",
      "5 Train accuracy: 0.98 Test accuracy: 0.9468\n",
      "6 Train accuracy: 0.94 Test accuracy: 0.9522\n",
      "7 Train accuracy: 0.96 Test accuracy: 0.9543\n",
      "8 Train accuracy: 0.92 Test accuracy: 0.9572\n",
      "9 Train accuracy: 0.98 Test accuracy: 0.9595\n",
      "10 Train accuracy: 0.98 Test accuracy: 0.9612\n",
      "11 Train accuracy: 0.92 Test accuracy: 0.9643\n",
      "12 Train accuracy: 0.96 Test accuracy: 0.9646\n",
      "13 Train accuracy: 1.0 Test accuracy: 0.9647\n",
      "14 Train accuracy: 1.0 Test accuracy: 0.9657\n",
      "15 Train accuracy: 0.96 Test accuracy: 0.9671\n",
      "16 Train accuracy: 0.98 Test accuracy: 0.9683\n",
      "17 Train accuracy: 0.98 Test accuracy: 0.9689\n",
      "18 Train accuracy: 0.98 Test accuracy: 0.9691\n",
      "19 Train accuracy: 0.98 Test accuracy: 0.9708\n",
      "20 Train accuracy: 1.0 Test accuracy: 0.9701\n",
      "21 Train accuracy: 0.98 Test accuracy: 0.9714\n",
      "22 Train accuracy: 1.0 Test accuracy: 0.9722\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.9723\n",
      "24 Train accuracy: 1.0 Test accuracy: 0.973\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.9734\n",
      "26 Train accuracy: 0.98 Test accuracy: 0.9738\n",
      "27 Train accuracy: 0.98 Test accuracy: 0.9744\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.9746\n",
      "29 Train accuracy: 0.98 Test accuracy: 0.9729\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    #initialize all nodes in the graph\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            \n",
    "            #get batch i - examples and targets\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            #feed batch to network - all examples in a batch will run in parallel.\n",
    "            #populate the place holders in the graph\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        #train/test sets accuracy measurments\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels}) \n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADKCAYAAAC11LviAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEgFJREFUeJzt3X+MXWWdx/HPp9Pa0pbI9Kf0B+CKSoZa2masLlRsBbSy\nZtFkMRDWQGKsf0iCxs3K6h+6JiZs4o/VaEiqEDERkax0RVKtFTBdNWEZSkt/saVbW21TWpBOWiKd\n/vruH3PIjvg8zNy5597beeb9Spq59zNnznlO58x3zpz7vedxRAgAMPZN6PQAAAD1oKADQCEo6ABQ\nCAo6ABSCgg4AhaCgA0AhKOgAUAgKOgAUgoIOAIWY2MwX214t6ZuSuiR9LyLuer3lu7u7Y968ec1s\nslYTJrT295nthrabWz7n7NmzyTz37t/c+nPLj5V3Eef268yZM20eSevljp1Gj6lG19OoRo+pXJ47\nxhvV6LGfk1u+1cfazp07X4yI2cMtN+qCbrtL0nckXSfpgKQnbT8cETtzXzNv3jw98MADo91k7aZN\nm9bS9ed+OHLbnTgx/e3IHYwDAwPJ/NSpU8m8q6srmZ88eTKZ1/ULI6fRH+5Gi9CxY8caGs9YMH36\n9GQ+efLkZJ47pnLrmTJlSjLP/R/nvle5YypX+HLLnzhxoqHtNvqLrdECncv7+/uTeV0WL168fyTL\nNfPreLmkPRGxNyJOSnpA0g1NrA8A0IRmCvp8SX8c8vxAlf0F22ts99nuO3r0aBObAwC8npa/KBoR\nayOiNyJ6u7u7W705ABi3minoByUtHPJ8QZUBADqgmYL+pKS32n6z7TdIuknSw/UMCwDQqFF3uUTE\nadu3S9qgwbbFeyNiR20jAwA0pKk+9IhYL2l9TWMBADSBd4oCQCEo6ABQCAo6ABSCgg4AhaCgA0Ah\nKOgAUAgKOgAUgoIOAIWgoANAISjoAFAICjoAFIKCDgCFoKADQCEo6ABQCAo6ABSCgg4AhaCgA0Ah\nKOgAUAgKOgAUgoIOAIWgoANAISY288W290k6LumMpNMR0VvHoICUw4cPJ/P169cn8xtvvLGVw8Hr\nOH78eDLfsmVLMj9z5kwyv+KKK2ob03jQVEGvrIqIF2tYDwCgCVxyAYBCNFvQQ9IvbT9le01qAdtr\nbPfZ7jt69GiTmwMA5DRb0FdExDJJH5T0KdtXv3aBiFgbEb0R0dvd3d3k5gAAOU0V9Ig4WH08Immd\npOV1DAoA0LhRvyhqe5qkCRFxvHr8fklfrm1kKN6JEyeS+c6dO5P57t27k/nmzZuTOV0urbdnz55k\n/vTTTyfzl156KZlPnJguRXS5NKaZLpe5ktbZfnU990fEL2oZFQCgYaMu6BGxVxK/PgHgHEHbIgAU\ngoIOAIWgoANAIep46z/GiNwbu/r7+5P53r17k3muq2Tfvn3JPHf/jv379yfzs2fPJvPzzz8/ma9c\nuTKZoz657/n27duTee7eLFUTxV9505veNLqBNekPf/hDMt+wYUMyz/1MfOYzn6ltTM3gDB0ACkFB\nB4BCUNABoBAUdAAoBAUdAApBl8sYMDAwkMxPnTqVzHP3SPnWt76VzH/xi/QdG3IzBOW6TWbNmpXM\nr7zyymT+hS98IZm/853vbGi7Eyakz0uOHTuWzJGX62Z59tlna1n/1KlTk/miRYtqWX9O7p4zP//5\nz5P5wYMHk/nJkydrG1MrcIYOAIWgoANAISjoAFAICjoAFIKCDgCFoMvlHJJ7ZX3dunXJfOHChcn8\n2muvTeYXX3xxMr/pppuSeW62mHe9613JPHefjpyIaChHffr6+pJ57v46jTrvvPOS+YoVK5L59OnT\nk3muYysn143z61//Opnn7jkzVnGGDgCFoKADQCEo6ABQCAo6ABRi2IJu+17bR2xvH5LNsL3R9nPV\nx+7WDhMAMJyRdLl8X9K3Jf1gSHanpEcj4i7bd1bPP1f/8MqUe2V9586dDa1n2bJlDS1/2223JfPc\n/SlyMwfRhTJ25Lo+du3alcxz3Sm5Dqb58+cn856enmT+xje+MZk3eo+UXPfLk08+mcwb7WZZsGBB\nMu/t7W1oPe027Bl6RGyS9NJr4hsk3Vc9vk/Sh2seFwCgQaO9hj43Ig5Vj5+XNLem8QAARqnpF0Vj\n8O/v7N/gttfY7rPdl5ukGADQvNEW9MO2L5Sk6uOR3IIRsTYieiOit7ub104BoFVGW9AflnRr9fhW\nST+tZzgAgNEatsvF9o8krZQ0y/YBSV+UdJekB21/XNJ+SR9t5SBLk5s9JTdD0Lx585L5zJkzkzld\nKOPP+vXrk/mRI+k/nmfMmNHQ+i+//PJkvnjx4mSe6ypptNvklVdeSeY/+9nPkvmhQ4eS+bRp0xra\nbq6DbOnSpcm8v7+/ofW3yrAFPSJuznzqmprHAgBoAu8UBYBCUNABoBAUdAAoBAUdAArBjEUdkJst\nJveKfm6moYkT09++U6dOjW5gOOc99thjyfx3v/tdMs91SOW6XHL3ZlmyZEkyz93jpdFulty9WbZu\n3ZrMf//73yfzCRMaO0fN3btm0aJFDa3nXMEZOgAUgoIOAIWgoANAISjoAFAICjoAFIIulw44ffp0\nQ8tfdtllLRoJxprbb789md944421rD/XnfL8888n89zMRy+//HIyz81MlLuP0e7du5N5V1dXMs/J\ndYStWLEimU+dOjWZN9q9026coQNAISjoAFAICjoAFIKCDgCFoKADQCHocmmhgYGBZH7s2LFknruv\nRG4WlrNnzybz3Cv6jc7agnPP/fffn8xz93JpVK6bJdeFkpsdK9cNkutyqWsC+dmzZyfzBQsWJPOe\nnp5kfq53s+Rwhg4AhaCgA0AhKOgAUAgKOgAUYtiCbvte20dsbx+Sfcn2Qdtbqn/Xt3aYAIDhjKTL\n5fuSvi3pB6/JvxERX619RAWZPHlyMs/dmyXXYbBhw4ZkfvDgwWSe62Z5xzvekcw/8IEPJPPcbDTo\nnNzMQblZqp555plWDqdjZs6cmcyvueaaZJ6b9SvXpTNWDXuGHhGbJL3UhrEAAJrQzDX0220/U12S\n6a5tRACAURltQb9b0lskLZF0SNLXcgvaXmO7z3ZfXW8eAAD8tVEV9Ig4HBFnIuKspO9KWv46y66N\niN6I6O3u5kQeAFplVAXd9oVDnn5E0vbcsgCA9hi2y8X2jyStlDTL9gFJX5S00vYSSSFpn6RPtnCM\nxbnuuuuSee4eLFu3bk3muft3bN+e/v26adOmZJ7rxlm1alUyx7knd0+S3P2BTpw40dDyuRmC/vSn\nPyXzuu6FMmPGjGR+8803J/Pp06fXst2xatiCHhGp/7l7WjAWAEATeKcoABSCgg4AhaCgA0AhKOgA\nUAhmLBoDcvfveO6555J5rmtl6dKlyTx3/4vcjEg49+S6QebMmZPMc90gU6ZMSea5rpVHHnkkmff3\n9yfz3L1Tch1ey5en3+IyderUZD7ecYYOAIWgoANAISjoAFAICjoAFIKCDgCFoMvlHPLKK68k81wn\nwW9/+9tkfvXVVyfz669PzxR48uTJEYwO40Fu5qPcfYOOHz/e0Ppzs2BdddVVyfxtb3tbMi9tpqG6\ncIYOAIWgoANAISjoAFAICjoAFIKCDgCFoMulhV544YVkvm3btmS+e/fuZJ7rPPjEJz6RzC+++OIR\njA74a3v37k3mBw4cqGX9s2fPTuaXXnppLesf7zhDB4BCUNABoBAUdAAoBAUdAAoxbEG3vdD247Z3\n2t5h+44qn2F7o+3nqo/drR8uACBnJF0upyV9NiI22z5f0lO2N0q6TdKjEXGX7Tsl3Snpc60bav1y\nr9xPmzYtmR89ejSZ/+pXv0rmDz30UDK//PLLk/ktt9ySzN/znvck85xcVwzwqtOnTyfzHTt21LL+\nWbNmJfMrr7wymXd1dSXzEydO1DKe8WLYM/SIOBQRm6vHxyXtkjRf0g2S7qsWu0/Sh1s1SADA8Bq6\nhm77EklLJT0haW5EHKo+9bykubWODADQkBEXdNvTJf1E0qcj4tjQz8XgvSyT97O0vcZ2n+2+3CUL\nAEDzRlTQbU/SYDH/YUS8emH4sO0Lq89fKOlI6msjYm1E9EZEb3c3r5sCQKuMpMvFku6RtCsivj7k\nUw9LurV6fKukn9Y/PADASI2ky+UqSR+TtM32lir7vKS7JD1o++OS9kv6aGuG2DrXXnttMs91AORm\nScn95bF69epknrsHS09PTzKfPHlyMh8YGEjmwHA2bdqUzP/85z8n89xMQxdccEEyf+9735vMJ05M\nlxxmzarHsAU9In4jKf3dlK6pdzgAgNHinaIAUAgKOgAUgoIOAIWgoANAIcb1jEV33313Mp80aVIy\nnzNnTjK/6KKLkvnUqVOTee5eMbkOAKBuL774YjKfMKGxc7y3v/3tyTzXmXXmzJmG1o/GcIYOAIWg\noANAISjoAFAICjoAFIKCDgCFGNdtFatWrer0EIAxIXcvl9mzZ7d5JHg9nKEDQCEo6ABQCAo6ABSC\ngg4AhaCgA0AhxnWXCzBeLVq0KJnv2bMnmc+fPz+Z52brYgaizuAMHQAKQUEHgEJQ0AGgEBR0ACjE\nsAXd9kLbj9veaXuH7Tuq/Eu2D9reUv27vvXDBQDkjKTL5bSkz0bEZtvnS3rK9sbqc9+IiK+2bngA\nWqGnpyeZL1u2LJnnZjKKiNrGhOYNW9Aj4pCkQ9Xj47Z3SUr3MAEAOqaha+i2L5G0VNITVXS77Wds\n32s73ZAKAGiLERd029Ml/UTSpyPimKS7Jb1F0hINnsF/LfN1a2z32e47evRoDUMGAKSMqKDbnqTB\nYv7DiHhIkiLicESciYizkr4raXnqayNibUT0RkRv7l1lAIDmjaTLxZLukbQrIr4+JL9wyGIfkbS9\n/uEBAEZqJF0uV0n6mKRttrdU2ecl3Wx7iaSQtE/SJ1syQgDAiIyky+U3klLzT62vfzgAgNHinaIA\nUAgKOgAUgoIOAIWgoANAISjoAFAICjoAFIKCDgCFoKADQCEo6ABQCAo6ABTC7ZxxxPYLkvZXT2dJ\nerFtG+889rdc42lfJfa3Ey6OiNnDLdTWgv4XG7b7IqK3IxvvAPa3XONpXyX291zGJRcAKAQFHQAK\n0cmCvraD2+4E9rdc42lfJfb3nNWxa+gAgHpxyQUACtH2gm57te3/sb3H9p3t3n472L7X9hHb24dk\nM2xvtP1c9bGIGbNtL7T9uO2dtnfYvqPKS93fKbb/2/bWan//tcrfbPuJ6rj+se03dHqsdbHdZftp\n249Uz0ve1322t9neYruvysbMsdzWgm67S9J3JH1QUo8G5yXtaecY2uT7kla/JrtT0qMR8VZJj1bP\nS3Ba0mcjokfSuyV9qvqelrq/A5LeFxFXSFoiabXtd0v6N0nfiIhLJR2V9PEOjrFud0jaNeR5yfsq\nSasiYsmQVsUxcyy3+wx9uaQ9EbE3Ik5KekDSDW0eQ8tFxCZJL70mvkHSfdXj+yR9uK2DapGIOBQR\nm6vHxzX4gz9f5e5vRMTL1dNJ1b+Q9D5J/1Hlxeyv7QWS/k7S96rnVqH7+jrGzLHc7oI+X9Ifhzw/\nUGXjwdyIOFQ9fl7S3E4OphVsXyJpqaQnVPD+Vpcgtkg6ImmjpP+V1B8Rp6tFSjqu/13SP0s6Wz2f\nqXL3VRr85fxL20/ZXlNlY+ZYntjpAYxHERG2i2ovsj1d0k8kfToijg2eyA0qbX8j4oykJbYvkLRO\n0mUdHlJL2P6QpCMR8ZTtlZ0eT5usiIiDtudI2mj72aGfPNeP5XafoR+UtHDI8wVVNh4ctn2hJFUf\nj3R4PLWxPUmDxfyHEfFQFRe7v6+KiH5Jj0v6W0kX2H71BKmU4/oqSX9ve58GL4++T9I3Vea+SpIi\n4mD18YgGf1kv1xg6lttd0J+U9NbqVfI3SLpJ0sNtHkOnPCzp1urxrZJ+2sGx1Ka6pnqPpF0R8fUh\nnyp1f2dXZ+ayfZ6k6zT4usHjkv6hWqyI/Y2If4mIBRFxiQZ/Vh+LiFtU4L5Kku1pts9/9bGk90va\nrjF0LLf9jUW2r9fgdbkuSfdGxFfaOoA2sP0jSSs1eJe2w5K+KOk/JT0o6SIN3nHyoxHx2hdOxxzb\nKyT9l6Rt+v/rrJ/X4HX0Evd3sQZfGOvS4AnRgxHxZdt/o8Gz2BmSnpb0jxEx0LmR1qu65PJPEfGh\nUve12q911dOJku6PiK/YnqkxcizzTlEAKATvFAWAQlDQAaAQFHQAKAQFHQAKQUEHgEJQ0AGgEBR0\nACgEBR0ACvF/6RZMREndjAwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x101d7f550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use the network on a \"new\" examples\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Get 2 digits from the dataset\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "target = mnist.target.reshape(mnist.target.shape[0], 1)\n",
    "data = mnist.data\n",
    "\n",
    "digit_5 = scaler.fit_transform(data[35000].reshape(28, 28))\n",
    "digit_4 = scaler.fit_transform(data[29999].reshape(28, 28))\n",
    "digits = np.c_[digit_5, digit_4]\n",
    "\n",
    "plt.imshow(digits, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "prediction:  [4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    digit4_new_scaled = digit_4.reshape(1,784)\n",
    "    Z = logits.eval(feed_dict={X: digit4_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "    print ('prediction: ', y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
