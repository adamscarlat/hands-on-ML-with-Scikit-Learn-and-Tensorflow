{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers\n",
    "-------\n",
    "1. Example applications for the different RNN architecture types:\n",
    "  - Sequence to Sequence:\n",
    "    * Translation from one language to another - the input is a sequence of tokens in language 1 and the output is a sequence\n",
    "      tokens in language 2.\n",
    "  - Sequence to vector:\n",
    "    * Forecasting the next time step in a series of time steps - the input is a sequence of time steps (e.g how many people took\n",
    "      the train every day for 13 days) and the output is the forecast for the 14th day.\n",
    "  - Vector to sequence:\n",
    "    * Getting a text label for an image - the input is an image (vector) and the output is a sequence of tokens saying what's in\n",
    "      the image. At each time step we input the same image and previous token generated.\n",
    "\n",
    "2. An RNN layer has the following dimensions \n",
    "  * Input:\n",
    "    - batch size\n",
    "    - number of step\n",
    "    - dimensionality: how many sequences in the data to consider\n",
    "  * Output:\n",
    "    - batch size\n",
    "    - vector to sequence: either return the last forecast (vector) or a sequence of forecasts.\n",
    "\n",
    "3. In a deep seq to seq RNN, we'd need all RNN layers to return sequences.\n",
    "\n",
    "4. If we have a daily univariate time series and we want to forecast the next 7 days, we can use:\n",
    "  - A seq to seq network\n",
    "    * We set the training data to be tuples of (train seq, next 7 labels)\n",
    "    * The network outputs a sequence which is the next 7 days\n",
    "  - A seq to vector network\n",
    "    * Here we set the training data to be tuples of (train seq, next day)\n",
    "    * The network outputs the next day forecast. We append these forecasts to a list and re-run inference in a loop\n",
    "      7 times to get the 7 day forecast.\n",
    "\n",
    "5. Main difficulties in training RNNs are:\n",
    "  - Unstable gradients and memory issues\n",
    "    * As the input sequences grow larger, the RNN cells lose information from the beginning of every sequence.\n",
    "    * To handle these, it's recommended to use better RNN cells like LSTM and GRU.\n",
    "    * Another way to handle it is using a 1D conv filter to shorten long sequences by concentrating them.\n",
    "  - Performance\n",
    "    * RNN cells must work sequentially on every input (since they depend on previous outputs t-1 at time t)\n",
    "\n",
    "6. Sketch an LSTM architecture\n",
    "\n",
    "7. Using a 1D conv layer in an RNN helps with long input sequences. If used before the RNN layer, it can shorten long sequences\n",
    "  using a \"valid\" padding and a stride greater than 1. This is a technique to tackle the memory problem RNNs have when dealing\n",
    "  with longer sequences.\n",
    "\n",
    "8. ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - Bach chorales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
