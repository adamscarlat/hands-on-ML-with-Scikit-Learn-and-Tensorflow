{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers\n",
    "-------\n",
    "1. Computation graph framework that allows building complex functions efficiently and in parallel.\n",
    "  TF serves as the backend for Keras. Another deep learning framework like TF is PyTorch\n",
    "\n",
    "2. TF and NP share many functions, especially around mathematical computation. However, TF serves as a heavy duty computation\n",
    "  framework that is meant for large mathematical models. It offers distributed computation for example and has a tight \n",
    "  integration with the GPU. Therefore, for simple operations, numpy is preferred.\n",
    "\n",
    "3. No, numpy works with 64 bit variable by default whereas tf works with 32 bit variables by default. The second one\n",
    "  will generate a tensor of type int64 and the first one a tensor of int32\n",
    "\n",
    "4. String, arrays, sparse tensors, ragged tensors, sets and queues\n",
    "\n",
    "5. Using Keras' Loss as a parent class offers built-in facilities such as tracking and serialization which are useful when\n",
    "  building a model.\n",
    "\n",
    "6. For tracking (?)\n",
    "\n",
    "7. Custom layers can be used when you need to define a custom operation inside a given layer and then use it in a model. It \n",
    "  does not control the flow of the model. Using a custom model is useful when you need to control the flow of information \n",
    "  between layers. For example, you may want to call a given layer 3 times during training.\n",
    "\n",
    "8. If we need to have more control over which layers affect the loss function or which parameters should be taken\n",
    "  into account during backpropagation.\n",
    "\n",
    "9. Yes, Keras components can contain arbitrary python code. It'll get converted to tf functions automatically.\n",
    "\n",
    "10. ?\n",
    "\n",
    "11. ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n",
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print (tf.range(10))\n",
    "print (tf.constant(np.arange(10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# train, validation and test split\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255\n",
    "\n",
    "num_classes = len(np.unique(y_train_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "\n",
    "Custom layer that performs layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.eps = 0.001\n",
    "  \n",
    "  def build(self, input_shape):\n",
    "    super().build(input_shape)\n",
    "    self.alpha = self.add_weight(\"alpha\", input_shape[-1:], dtype=\"float32\", initializer=\"ones\")\n",
    "    self.beta = self.add_weight(\"beta\", input_shape[-1:], dtype=\"float32\", initializer=\"zeros\")\n",
    "  \n",
    "  def call(self, X):\n",
    "    mean, var = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "    std = tf.sqrt(var)\n",
    "\n",
    "    return self.alpha * ((X - mean) / (std + self.eps)) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " normalized_layer_2 (Normal  (None, 784)               1568      \n",
      " izedLayer)                                                      \n",
      "                                                                 \n",
      " normalized_layer_3 (Normal  (None, 784)               1568      \n",
      " izedLayer)                                                      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10986 (42.91 KB)\n",
      "Trainable params: 10986 (42.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        NormalizedLayer(),\n",
    "        NormalizedLayer(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 1s 600us/step - loss: 0.4009 - accuracy: 0.8601 - val_loss: 0.4028 - val_accuracy: 0.8602\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 1s 566us/step - loss: 0.3963 - accuracy: 0.8612 - val_loss: 0.4078 - val_accuracy: 0.8582\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 1s 544us/step - loss: 0.3935 - accuracy: 0.8620 - val_loss: 0.4217 - val_accuracy: 0.8528\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 1s 537us/step - loss: 0.3914 - accuracy: 0.8627 - val_loss: 0.4089 - val_accuracy: 0.8586\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 1s 562us/step - loss: 0.3880 - accuracy: 0.8647 - val_loss: 0.4101 - val_accuracy: 0.8544\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm 0:  783.2976 4.2915344e-06\n",
      "Norm 1:  785.9258 -0.009297669\n"
     ]
    }
   ],
   "source": [
    "norm_0 = model.get_layer(\"normalized_layer\")\n",
    "norm_1 = model.get_layer(\"normalized_layer_1\")\n",
    "\n",
    "weights_0, biases_0 = norm_0.get_weights()\n",
    "print (\"Norm 0: \", weights_0.sum(), biases_0.sum())\n",
    "\n",
    "weights_1, biases_1 = norm_1.get_weights()\n",
    "print (\"Norm 1: \", weights_1.sum(), biases_1.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " layer_normalization_4 (Lay  (None, 784)               1568      \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_5 (Lay  (None, 784)               1568      \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10986 (42.91 KB)\n",
      "Trainable params: 10986 (42.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Comparing it to Keras LayerNormalization layer which does the same thing\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 1s 619us/step - loss: 0.5789 - accuracy: 0.7981 - val_loss: 0.4648 - val_accuracy: 0.8324\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 1s 570us/step - loss: 0.4682 - accuracy: 0.8369 - val_loss: 0.4480 - val_accuracy: 0.8422\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 1s 559us/step - loss: 0.4452 - accuracy: 0.8440 - val_loss: 0.4432 - val_accuracy: 0.8436\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 1s 553us/step - loss: 0.4309 - accuracy: 0.8506 - val_loss: 0.4262 - val_accuracy: 0.8518\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 1s 559us/step - loss: 0.4206 - accuracy: 0.8531 - val_loss: 0.4137 - val_accuracy: 0.8538\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm 0:  784.0 0.0\n",
      "Norm 1:  784.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Comparing the sum of weights and biases we got to the custom normalization layer\n",
    "\n",
    "norm_0 = model.get_layer(\"layer_normalization_4\")\n",
    "norm_1 = model.get_layer(\"layer_normalization_5\")\n",
    "\n",
    "weights_0, biases_0 = norm_0.get_weights()\n",
    "print (\"Norm 0: \", weights_0.sum(), biases_0.sum())\n",
    "\n",
    "weights_1, biases_1 = norm_1.get_weights()\n",
    "print (\"Norm 1: \", weights_1.sum(), biases_1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13\n",
    "\n",
    "Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1718/1718 - mean_loss: 0.711555004119873 - accuracy: 0.77004510164260869\n",
      "Validation metrics: \n",
      "1718/1718 - valid_loss: 0.4127207100391388 - accuracy: 0.7701061367988586\n",
      "Epoch 2/5\n",
      "1436/1718 - mean_loss: 0.4920012354850769 - accuracy: 0.82990944385528568"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch12-tensorflow/exercises.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch12-tensorflow/exercises.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m   mean_loss(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch12-tensorflow/exercises.ipynb#X20sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m   \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch12-tensorflow/exercises.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     metric(np\u001b[39m.\u001b[39;49margmax(y_batch, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), np\u001b[39m.\u001b[39;49margmax(y_pred\u001b[39m.\u001b[39;49mnumpy(), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch12-tensorflow/exercises.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m   print_status_bar(step, n_steps, mean_loss, metrics)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch12-tensorflow/exercises.ipynb#X20sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Print validation accuracy\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py:216\u001b[0m, in \u001b[0;36mMetric.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[39mreturn\u001b[39;00m result_t\n\u001b[1;32m    212\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    213\u001b[0m     distributed_training_utils,\n\u001b[1;32m    214\u001b[0m )\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m distributed_training_utils\u001b[39m.\u001b[39;49mcall_replica_local_fn(\n\u001b[1;32m    217\u001b[0m     replica_local_fn, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    218\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/distribute/distributed_training_utils.py:62\u001b[0m, in \u001b[0;36mcall_replica_local_fn\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mwith\u001b[39;00m strategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m     61\u001b[0m         \u001b[39mreturn\u001b[39;00m strategy\u001b[39m.\u001b[39mextended\u001b[39m.\u001b[39mcall_for_each_replica(fn, args, kwargs)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py:187\u001b[0m, in \u001b[0;36mMetric.__call__.<locals>.replica_local_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m     update_op \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     update_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_state(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    188\u001b[0m update_ops \u001b[39m=\u001b[39m []\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m update_op \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py:77\u001b[0m, in \u001b[0;36mupdate_state_wrapper.<locals>.decorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTrying to run metric.update_state in replica context when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe metric was not created in TPUStrategy scope. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMake sure the keras Metric is created in TPUstrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mscope. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m tf_utils\u001b[39m.\u001b[39mgraph_context_for_symbolic_tensors(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 77\u001b[0m     result \u001b[39m=\u001b[39m update_state_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m     79\u001b[0m     result \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39mget_operations()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py:140\u001b[0m, in \u001b[0;36mMetric.__new__.<locals>.update_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m control_status \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    137\u001b[0m ag_update_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    138\u001b[0m     obj_update_state, control_status\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m ag_update_state(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py:728\u001b[0m, in \u001b[0;36mMeanMetricWrapper.update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(matches)\n\u001b[1;32m    725\u001b[0m sample_weight \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mapply_valid_mask(\n\u001b[1;32m    726\u001b[0m     matches, sample_weight, mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction\n\u001b[1;32m    727\u001b[0m )\n\u001b[0;32m--> 728\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mupdate_state(matches, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/metrics/base_metric.py:539\u001b[0m, in \u001b[0;36mReduce.update_state\u001b[0;34m(self, values, sample_weight)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction \u001b[39m==\u001b[39m metrics_utils\u001b[39m.\u001b[39mReduction\u001b[39m.\u001b[39mWEIGHTED_MEAN:\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m         num_values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tf\u001b[39m.\u001b[39;49msize(values), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype)\n\u001b[1;32m    540\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    541\u001b[0m         num_values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(sample_weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1261\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:808\u001b[0m, in \u001b[0;36msize_v2\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    806\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     out_type \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mint32\n\u001b[0;32m--> 808\u001b[0m \u001b[39mreturn\u001b[39;00m size(\u001b[39minput\u001b[39;49m, name, out_type)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1261\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:847\u001b[0m, in \u001b[0;36msize\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    845\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    846\u001b[0m     out_type \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mint32\n\u001b[0;32m--> 847\u001b[0m \u001b[39mreturn\u001b[39;00m size_internal(\u001b[39minput\u001b[39;49m, name, optimize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, out_type\u001b[39m=\u001b[39;49mout_type)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:871\u001b[0m, in \u001b[0;36msize_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    869\u001b[0m   np_out_type \u001b[39m=\u001b[39m out_type\u001b[39m.\u001b[39mas_numpy_dtype\n\u001b[1;32m    870\u001b[0m   num_elements \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mprod(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39m_shape_tuple(), dtype\u001b[39m=\u001b[39mnp_out_type)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m   \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(num_elements, dtype\u001b[39m=\u001b[39;49mout_type)\n\u001b[1;32m    872\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(name, \u001b[39m\"\u001b[39m\u001b[39mSize\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39minput\u001b[39m]) \u001b[39mas\u001b[39;00m name:\n\u001b[1;32m    873\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    874\u001b[0m       \u001b[39minput\u001b[39m, (sparse_tensor\u001b[39m.\u001b[39mSparseTensor, sparse_tensor\u001b[39m.\u001b[39mSparseTensorValue)):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:698\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    697\u001b[0m preferred_dtype \u001b[39m=\u001b[39m preferred_dtype \u001b[39mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 698\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m    699\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[1;32m    700\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:252\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, accepted_result_types):\n\u001b[1;32m    247\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m       _add_error_prefix(\n\u001b[1;32m    249\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbase_type\u001b[39m}\u001b[39;00m\u001b[39m returned non-Tensor: \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    251\u001b[0m           name\u001b[39m=\u001b[39mname))\n\u001b[0;32m--> 252\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m dtype\u001b[39m.\u001b[39;49mis_compatible_with(ret\u001b[39m.\u001b[39;49mdtype):\n\u001b[1;32m    253\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    254\u001b[0m       _add_error_prefix(\n\u001b[1;32m    255\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m{\u001b[39;00mbase_type\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreturned incompatible dtype: requested = \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    258\u001b[0m           name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/dtypes.py:228\u001b[0m, in \u001b[0;36mDType.is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_compatible_with\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    211\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns True if the `other` DType will be converted to this DType (TF1).\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[39m  Programs written for TensorFlow 2.x do not need this function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m    this `DType`.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m   other \u001b[39m=\u001b[39m as_dtype(other)\n\u001b[1;32m    229\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type_enum \u001b[39min\u001b[39;00m (other\u001b[39m.\u001b[39mas_datatype_enum,\n\u001b[1;32m    230\u001b[0m                              other\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mas_datatype_enum)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/dtypes.py:799\u001b[0m, in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[39m# Ensure no collisions.\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(_ANY_TO_TF) \u001b[39m==\u001b[39m \u001b[39msum\u001b[39m(\n\u001b[1;32m    796\u001b[0m     \u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m [_INTERN_TABLE, _STRING_TO_TF, _PYTHON_TO_TF, _NP_TO_TF])\n\u001b[0;32m--> 799\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdtypes.as_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mas_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    800\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mas_dtype\u001b[39m(type_value):\n\u001b[1;32m    801\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts the given `type_value` to a `tf.DType`.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \n\u001b[1;32m    803\u001b[0m \u001b[39m  Inputs can be existing `tf.DType` objects, a [`DataType`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m    TypeError: If `type_value` cannot be converted to a `DType`.\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    831\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(type_value, DType):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "def random_batch(X, y, batch_size=32):\n",
    "  idx = np.random.randint(len(X), size=batch_size)\n",
    "  X_batch = X[idx]\n",
    "  y_batch = y[idx]\n",
    "  y_batch_one_hot = tf.keras.utils.to_categorical(y_batch, num_classes=num_classes)\n",
    "  return X_batch, y_batch_one_hot\n",
    "\n",
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "  metrics = \" - \".join([f\"{m.name}: {m.result()}\" for m in [loss] + (metrics or [])])\n",
    "  end = \"\" if step < total else \"\\n\"\n",
    "  print (f\"\\r{step}/{total} - \" + metrics, end=end)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "valid_loss = tf.keras.metrics.Mean(name=\"valid_loss\")\n",
    "metrics = [tf.keras.metrics.Accuracy()]\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  print (\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "  for step in range(1, n_steps + 1):\n",
    "    X_batch, y_batch = random_batch(X_train, y_train)\n",
    "    with tf.GradientTape() as tape:\n",
    "      # Forward pass\n",
    "      y_pred = model(X_batch, training=True)\n",
    "      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "      loss = tf.add_n([main_loss] + model.losses)\n",
    "    \n",
    "    # Backpropagation\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    mean_loss(loss)\n",
    "    for metric in metrics:\n",
    "      metric(np.argmax(y_batch, axis=1), np.argmax(y_pred.numpy(), axis=1))\n",
    "    \n",
    "    print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "  \n",
    "\n",
    "  # Print validation accuracy\n",
    "  X_valid_batch, y_valid_batch = random_batch(X_valid, y_valid)\n",
    "  y_valid_pred = model(X_valid_batch, training=True)\n",
    "  valid_main_loss = tf.reduce_mean(loss_fn(y_valid_batch, y_valid_pred))\n",
    "  valid_loss_n = tf.add_n([valid_main_loss] + model.losses)\n",
    "  valid_loss(valid_loss_n)\n",
    "\n",
    "  for metric in metrics:\n",
    "    metric(np.argmax(y_batch, axis=1), np.argmax(y_pred.numpy(), axis=1)) \n",
    "\n",
    "  print (\"Validation metrics: \")\n",
    "  print_status_bar(step, n_steps, valid_loss, metrics)\n",
    "\n",
    "  for metric in [mean_loss] + metrics:\n",
    "    metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
