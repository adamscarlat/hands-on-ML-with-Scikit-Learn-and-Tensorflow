{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "<dtype: 'float32'>\n",
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.]\n",
      " [5.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[14. 32.]\n",
      " [32. 77.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example - a (2,3) matrix\n",
    "t = tf.constant([\n",
    "    [1., 2., 3.], \n",
    "    [4., 5., 6.]\n",
    "  ])\n",
    "print (t.shape)\n",
    "print (t.dtype)\n",
    "\n",
    "# Indexing (same as numpy)\n",
    "print (t[: ,1:])\n",
    "\n",
    "# ... means all preceding dimensions (all rows)\n",
    "# 1 - second column\n",
    "# tf.newaxis - add a new dimension to the result\n",
    "print (t[..., 1, tf.newaxis])\n",
    "\n",
    "# addition example (not in-place, returns a result tensor)\n",
    "print (t + 10)\n",
    "\n",
    "# dot product\n",
    "print (t @ tf.transpose(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 5.], shape=(3,), dtype=float64)\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "tf.Tensor([ 4. 16. 25.], shape=(3,), dtype=float64)\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]]\n"
     ]
    }
   ],
   "source": [
    "# Switching between tensor to numpy types and vise versa\n",
    "\n",
    "# From numpy to tensorflow\n",
    "a = np.array([2., 4., 5.])\n",
    "print (tf.constant(a))\n",
    "\n",
    "# From tensorflow to numpy\n",
    "t = tf.constant([\n",
    "    [1., 2., 3.], \n",
    "    [4., 5., 6.]\n",
    "  ])\n",
    "print (t.numpy())\n",
    "\n",
    "# Operations on mixed types work as well\n",
    "# tensorflow functions on numpy types\n",
    "print (tf.square(a))\n",
    "\n",
    "# numpy functions on tensorflow types\n",
    "print (np.square(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversions\n",
    "\n",
    "Type match is important in tensorflow when doing any operation (casting is bad for performance and TF does not want\n",
    "it to go unnoticed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to add float32 (tf default float) and float64 results in an error\n",
    "tf.constant(2.) + tf.constant(4., dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "All the tensors we've seen so far are immutable. This means that we cannot use them to implement weights in a NN,\n",
    "since they have to be tweaked when doing backpropagation.\n",
    "\n",
    "TF variables are like tensors, they work with all the tf functions we've seen. The main difference is that they support \n",
    "in-place operations using the `assign()` method and friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 2.,  4.,  6.],\n",
      "       [ 8., 10., 12.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[  2., 104., 106.],\n",
      "       [  8., 110., 112.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[100., 104., 106.],\n",
      "       [  8., 110., 200.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable([\n",
    "    [1., 2., 3.], \n",
    "    [4., 5., 6.]\n",
    "  ])\n",
    "\n",
    "# Happens in-place!\n",
    "\n",
    "# Multiply example\n",
    "v.assign(2 * v)\n",
    "print (v)\n",
    "\n",
    "# Work on subsection of the tensor in-place\n",
    "v[:,1:].assign(v[:,1:] + 100)\n",
    "print (v)\n",
    "\n",
    "# Work on subsection of the tensor in-place (another approach better for cell updates)\n",
    "v.scatter_nd_update(\n",
    "  indices=[[0, 0], [1, 2]],\n",
    "  updates=[100., 200.]\n",
    ")\n",
    "print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms\n",
    "\n",
    "### Customizing Loss Functions\n",
    "\n",
    "Suppose that you have a regression task and that your dataset is ridden with outliers. The MSE loss function penalizes large errors\n",
    "too much, causing the model to become imprecise. The MAE loss doesn't penalize the outliers enough and training takes too long\n",
    "to converge.\n",
    "\n",
    "Instead you decide to implement the Huber loss (which is available in Keras btw). This loss function is mix between MSE and MAE:\n",
    "* It computes the error on the given batch (e.g. error = y_true - y_pred)\n",
    "* For small error values (less than 1), it does a squared loss\n",
    "* For large error values, it does a linear loss\n",
    "\n",
    "**These custom components are not saved automatically when serializing the model. See the book for how to save/load models \n",
    "  with custom components**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber loss function\n",
    "def huber_fn(y_true, y_pred):\n",
    "  # Get the residual vector\n",
    "  error = y_true - y_pred\n",
    "\n",
    "  # Make it a boolean vector, marking small and large residuals\n",
    "  is_small_error = tf.abs(error) < 1\n",
    "\n",
    "  # squared loss will be used for small residuals\n",
    "  squared_loss = tf.square(error) / 2\n",
    "  # linear loss will be used for large residuals\n",
    "  linear_loss = tf.abs(error) - 0.5\n",
    "\n",
    "  return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "\n",
    "# CODE DOES NOT RUN! \n",
    "# Use it in a Keras model\n",
    "model = tf.keras.Sequential([])\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber loss function with a configurable parameter\n",
    "def create_huber(threshold=1.0):\n",
    "  def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < threshold\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "  return huber_fn\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics\n",
    "\n",
    "### Streaming metrics\n",
    "\n",
    "These maintain the overall metric throughout the training. For example, the precision metrics below treats every input into \n",
    "it as a training batch result (true, pred). It maintains state of the precision so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.8, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "precision = tf.keras.metrics.Precision()\n",
    "\n",
    "# Emulating batch 1 results (80% precision)\n",
    "batch1_y_true = [0,1,1,1,0,1,0,1]\n",
    "batch1_y_pred = [1,1,0,1,0,1,0,1]\n",
    "print (precision(batch1_y_true, batch1_y_pred))\n",
    "\n",
    "# Emulating batch 2 results (0% precision). Note that it says 0.5 because it's the accumulated precision - the\n",
    "# overall precision between the two batches: \n",
    "#   Batch1 had 4 TP, 1 FP. \n",
    "#   Batch2 had 0 TP, 4 FP)\n",
    "# Precision = 4 / (4 + 4) = 0.5\n",
    "batch2_y_true = [0,1,0,0,1,0,1,1]\n",
    "batch2_y_pred = [1,0,1,1,0,0,0,0]\n",
    "print (precision(batch2_y_true, batch2_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Huber metric (taking a mean of the Huber loss and using it as a metric)\n",
    "\n",
    "def create_huber(threshold=1.0):\n",
    "  def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < threshold\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "  return huber_fn\n",
    "\n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "  def __init__(self, threshold=1.0, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.threshold = threshold\n",
    "    self.huber_fn = create_huber(threshold)\n",
    "    # Similar to a tf.Variable only that it's trackable and is specifically for weights storage \n",
    "    self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "    self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "  \n",
    "  # Update the metrics sum and count. Gets called first\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    sample_metrics = self.huber_fn(y_true, y_pred)\n",
    "    self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
    "    self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "  \n",
    "  # Computes the mean. Gets called after update_state\n",
    "  def result(self):\n",
    "    return self.total / self.count\n",
    "  \n",
    "  # For save/load model\n",
    "  def get_config(self):\n",
    "    base_config = super().get_config()\n",
    "    return {**base_config, \"threshold\": self.threshold}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "\n",
    "Good for building custom layers that Keras doesn't offer and also good for building a number of layers into a single layers\n",
    "block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build a no weights layer (e.g. Flatten or ReLU), we can use the Lambda layer in Keras.\n",
    "# For example, a layer that takes the exponent of its inputs (already available in Keras under activation=\"exponential\"...)\n",
    "\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a layer with weights. For example, building our own version of the Dense layer\n",
    "\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, activation=None, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.units = units\n",
    "    self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "  # Called once, the first time that the layer is used\n",
    "  def build(self, batch_input_shape):\n",
    "    # Define the layer's weight matrix\n",
    "    self.kernel = self.add_weight(\n",
    "      name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "      initializer=\"glorot_normal\"\n",
    "    )\n",
    "    self.bias = self.add_weight(name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "  \n",
    "  # Called when the layer is working during training and inference\n",
    "  def call(self, X):\n",
    "    return self.activation(X @ self.kernel + self.bias)\n",
    "  \n",
    "  # Used for serialization/deserialization of this custom layer\n",
    "  def get_config(self):\n",
    "    base_config = super().get_config()\n",
    "    return {**base_config, \"units\": self.units, \"activation\": tf.keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom implementation of the GaussianNoise layer that Keras offer. This layer adds noise during training but not \n",
    "# during inference. It's an example of a layer that has a different behavior during training and inference.\n",
    "\n",
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "  def __init__(self, stddev, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.stddev = stddev\n",
    "  \n",
    "  # Keras will pass the training parameter set to True during training and we can have a different \n",
    "  # behavior based on it.\n",
    "  def call(self, X, training=False):\n",
    "    if training:\n",
    "      noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "      return X + noise\n",
    "    else:\n",
    "      return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a model with a residual block - a block of two dense layers that adds its input to its output.\n",
    "# This residual block will get called 3 times\n",
    "\n",
    "# We start by creating a residual block layer that will contain multiple Dense layers\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "                   for _ in range(n_layers)]\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    Z = inputs\n",
    "    for layer in self.hidden:\n",
    "      Z = layer(Z)\n",
    "    return inputs + Z\n",
    "  \n",
    "# Next we define the custom model that will use this residual block\n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "  # We define the model's layer in the ctor\n",
    "  def __init__(self, output_dim, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "    self.block1 = ResidualBlock(2, 30)\n",
    "    self.block2 = ResidualBlock(2, 30)\n",
    "    self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "  # We define the model's layer to layer operations here\n",
    "  def call(self, inputs):\n",
    "    Z = self.hidden1(inputs)\n",
    "    for _ in range(1+3):\n",
    "      Z = self.block1(Z)\n",
    "    Z = self.block2(Z)\n",
    "    return self.out(Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Metrics that are based on the model's internals\n",
    "\n",
    "So far we've used loss and metrics that are based on the labels and predictions. We can define models where the loss and metrics\n",
    "are defined based on internal layers.\n",
    "\n",
    "One such model uses the \"reconstruction loss\" - it's the mean squared difference between the reconstruction and the inputs. It is\n",
    "used in auto-encoder models and encourages a model to preserve as much information as possible through the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "  def __init__(self, output_dim, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    # 5 hidden Dense layers\n",
    "    self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\") \n",
    "                   for _ in range(5)]\n",
    "    self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    # This will be used to keep track of teh reconstruction error during training\n",
    "    self.reconstruction_mean = tf.keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "  def build(self, batch_input_shape):\n",
    "    # Create an extra Dense layer for the reconstruction error. It must be done here (not in ctor)\n",
    "    # since its number of units must equal the input shape and this is not known until we call\n",
    "    # the model for the first time with inputs.\n",
    "    n_inputs = batch_input_shape[-1]\n",
    "    self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "  \n",
    "  def call(self, inputs, training=False):\n",
    "    # Process the inputs through the model's hidden layers\n",
    "    Z = inputs\n",
    "    for layer in self.hidden:\n",
    "      Z = layer(Z)\n",
    "\n",
    "    # Pass the result thru the recon layer and compute the recon error\n",
    "    reconstruction = self.reconstruct(Z)\n",
    "    recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "    # Scale down the recon error\n",
    "    self.add_loss(0.05 * recon_loss)\n",
    "\n",
    "    if training:\n",
    "      result = self.reconstruction_mean(recon_loss)\n",
    "      self.add_metric(result)\n",
    "    return self.out(Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradient Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n"
     ]
    }
   ],
   "source": [
    "# Working with a toy function to understand how to use autodiff\n",
    "\n",
    "def f(w1, w2):\n",
    "  return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "# Implementing reverse mode autodiff (using inputs 5, 3)\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.) \n",
    "\n",
    "# GradientTape records every operation that involves a variable.\n",
    "# Tape gets erased after calling this method unless we use (persistent=True)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  z = f(w1, w2)\n",
    "\n",
    "# Compute the gradient of result z with respect to inputs w1, w2.\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "\n",
    "print (gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
