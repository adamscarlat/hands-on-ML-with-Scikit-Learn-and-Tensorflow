{"cells":[{"cell_type":"markdown","metadata":{},"source":["Answers\n","-------\n","1. Advantages of a CNN over a fully connected DNN for image classification:\n","  - Less parameters. The conv layers are not fully connected, instead they are connected to smaller regions in the \n","    input layer and slide over it. This means:\n","    * Less computational resources\n","    * Less prone to overfitting\n","  - Locality\n","    * When a pattern is learned in one part of the image, it can be detected in another\n","      part of the image.\n","  - The input can remain 2D. There is no need to flatten it.\n","    * This helps with maintaining the structural integrity of images.\n","\n","2.  Assume an RGB image of size 200x300 inputted into a CNN with 3 conv layers, each with 3x3 filters, stride=2 and \"same\"\n","  padding.\n","  - First conv layer: outputs 100 feature maps\n","  - Second conv layer: outputs 200 feature maps\n","  - Third conv layer: outputs 400 feature maps\n","\n","  a. Number of parameters in this CNN:\n","  - First layer:\n","    * 100 filters x 3 x 3 x 3 channels (RGB) + 100 bias terms = 2,800 parameters\n","  - Second layer:\n","    * 200 filters x 3 x 3 x 100 channels (prev layer) + 200 bias terms = 180,200 parameters\n","  - Third layer:\n","    * 400 filters x 3 x 3 x 200 channels + 400 bias terms = 720,400\n","  - Total:\n","    * 2800 + 180200 + 720400 = 903,400 parameters\n","\n","  b. Assume we're using 32 bit floats. At least how much RAM will this CNN require to predict a single image?\n","    - Since we're only doing prediction, we can unload a conv layer after it's done so we'll compute the RAM \n","      needed per layer and take the max amount as the answer. Also, since the stride is 2 and padding is same,\n","      we know that the size of the feature maps is divided by 2 every time\n","    - First layer:\n","      * 2,800 parameters + (200 / 2 * 300 / 2) * 100 feature maps + 200 * 300 * 3 input image = 1,682,800 \n","      * Since each pixel is 4 bytes: 4 * 1,682,800 = 6,731,200 bytes ~ 5.45 MB\n","    - Second layer:\n","      * 180,200 + (100 / 2 * 150 / 2) * 200 + (200 / 2 * 300 / 2) * 100 = 2,430,200\n","      * 2,430,200 * 4 = 9,720,800 bytes ~ 9.3 MB\n","    - Third layer:\n","      * 720,400 + (50 / 2 * 75 / 2) * 400 + (100 / 2 * 150 / 2) * 200 = 1,845,400\n","      * 1,845,400 * 4 = 7,381,600 bytes ~ 7 MB\n","\n","  c. How much RAM will we need for training a batch of 50 such images?\n","    - Load all layers and their parameters into memory:\n","      5.45 + 9.3 + 7 = 21.75 MB\n","    - Multiply by 50:\n","      21.75 * 50 = 1,087MB\n","  \n","3. If the GPU memory runs out during training:\n","  - Reduce the batch size\n","  - Use larger stride size\n","  - Remove one or more layers\n","  - Use 16-bit floats\n","  - Distribute the training across multiple devices\n","\n","4. Max pooling helps by concentrating the strongest signal. It helps reducing the number of learnable parameters, providing\n","  support against overfitting and also provides faster computation. A max pooling layer has no parameters, therefore, it's\n","  more efficient.\n","\n","5. Local response normalization layer \n","\n","6. \n","\n","7. A fully convolutional layer (FCN) is a type of CNN with no dense layer at its top. Instead it uses convolutional layers.\n","  This makes the network tolerant to inputs of different sizes. To make a dense layer into a convolutional layer, we use a \n","  filter size that's the same as the input size and a stride of 1 with padding set to valid.\n","\n","8. Main difficulty with semantic segmentation is that CNNs learn to identify objects in an image regardless of their location.\n","  SS requires knowledge of the object's location in order to classify each pixel in the image.\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","\n","from functools import partial\n","\n","from sklearn.datasets import fetch_openml\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 9 - CNN on MNIST"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}],"source":["# Exercise 9 - CNN on MNIST\n","\n","mnist = fetch_openml(\"mnist_784\", as_frame=False)\n","X, y = mnist.data, mnist.target\n","X_train, X_test, y_train, y_test = X[:60000] / 255, X[60000:] / 255, y[:60000], y[60000:]\n","y_train, y_test = y_train.astype(float), y_test.astype(float)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", \n","                        activation=\"relu\", kernel_initializer=\"he_normal\")\n","\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Reshape(target_shape=[28,28,1]),\n","\n","  DefaultConv2D(filters=64, kernel_size=7, input_shape=[28,28,1]),\n","  tf.keras.layers.MaxPool2D(),\n","  DefaultConv2D(filters=128),\n","  DefaultConv2D(filters=128),\n","  DefaultConv2D(filters=256),\n","  tf.keras.layers.MaxPool2D(),\n","  DefaultConv2D(filters=256),\n","  DefaultConv2D(filters=256),\n","  tf.keras.layers.MaxPool2D(),\n","\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n","  tf.keras.layers.Dropout(0.5),\n","  \n","  tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n","  tf.keras.layers.Dropout(0.5),\n","\n","  tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n","])\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !!! Run this on Kaggle using a T4x2 GPU. Too slow for this machine...\n","\n","early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n","history = model.fit(X_train, y_train, epochs=30, validation_split=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 10 - transfer learning for large image classification"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset, info = tfds.load(\"caltech_birds2011\", \n","                          split=[\"train[:80%]\", \"train[80%:]\", \"test\"],\n","                          as_supervised=True, \n","                          with_info=True)\n","\n","dataset_size = info.splits[\"train\"].num_examples\n","class_names = info.features[\"label\"].names\n","n_classes = info.features[\"label\"].num_classes\n","\n","train_set_raw, valid_set_raw, test_set_raw = dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocess the images\n","\n","batch_size = 32\n","\n","# Resizing and using the Xception built-in preprocessing as a single Keras preprocessing model\n","preprocess = tf.keras.Sequential([\n","  tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n","  tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n","])\n","train_set = train_set_raw.map(lambda X,y: (preprocess(X), y))\n","\n","# Shuffle and batch the training set\n","train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n","\n","# Preprocess for validation and test sets\n","valid_set = valid_set_raw.map(lambda X,y: (preprocess(X), y))\n","valid_set = valid_set.batch(batch_size)\n","\n","test_set = test_set_raw.map(lambda X,y: (preprocess(X), y))\n","test_set = test_set.batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading the Xception model\n","\n","# We set include_top=False so that it excludes the global avg pooling and dense output layer.\n","# We'll add our own output softmax layer for the flowers labels\n","base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n","\n","# Adding our own \"top\" layers\n","dense1 = tf.keras.layers.Dense(800, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(base_model.output)\n","#dense2 = tf.keras.layers.Dense(400, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(dense1)\n","#dense3 = tf.keras.layers.Dense(400, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(dense2)\n","avg = tf.keras.layers.GlobalAveragePooling2D()(dense1)\n","output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n","model = tf.keras.Model(inputs=base_model.input, outputs=output)\n","\n","# Freezing the weights of the pretrained layers so that we don't corrupt them during training\n","for layer in base_model.layers:\n","  layer.trainable = False\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n","\n","#model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We start by doing 3 epochs on the new top with everything below it frozen\n","history = model.fit(train_set, validation_data=valid_set, epochs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fitting more layers - USE GPU! very slow\n","\n","# Now that we calibrated the top, we can unfreeze more layers below for training. The first calibration ensures \n","# that the large gradients don't corrupt the well trained layer weights\n","\n","for layer in base_model.layers[120:]:\n","  layer.trainable = True\n","\n","# Need to re-compile\n","# Notice that we decreased the learning rate also to not corrupt the unfrozen, well trained layers\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n","\n","# Training for longer\n","history = model.fit(train_set, validation_data=valid_set, epochs=10)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
