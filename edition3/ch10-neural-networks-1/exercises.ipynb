{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers (3-9)\n",
    "------------\n",
    "3. A single perceptron will not be able to learn a non-linear function. To use perceptron to learn a non-linear function,\n",
    "  we need to use a non-linear activation function.\n",
    "\n",
    "4. It's differentiable, therefore we can use it with gradient descent. In addition, its derivative is always non-zero, promising\n",
    "  an update of the weights and improving training.\n",
    "\n",
    "5. Sigmoid\n",
    "  ReLU\n",
    "  tanH\n",
    "\n",
    "6. MLP:\n",
    "  * Input: 10 \n",
    "  * Hidden 1: 50\n",
    "  * Output: 3\n",
    "  a. Shape of input matrix: (m, 10)\n",
    "  b. Shape of hidden layer:\n",
    "    b1. Weights matrix: (50, 10)\n",
    "    b2. Bias vector (50, 1)\n",
    "    X @ W.T + b = (m,10) @ (10,50) + (50,1) = (m,50) \n",
    "  c. Shape of output layer:\n",
    "    c1. Weights matrix: (3, 50)\n",
    "    c2. Bias vector: (3, 1)\n",
    "  d. Shape of output matrix:\n",
    "    (m, 50) @ (50, 3) = (m, 3)\n",
    "  e. Equation of network:\n",
    "    f(X,W1,b1,W2,b2) = a(a(X @ W1.T + b1) @ W2.T + b2)\n",
    "    \n",
    "7. For spam vs. ham, we'd need a single output neuron. It's result is the probability for spam (e.g. positive) or ham (1-probability).\n",
    "  Activation function should be sigmoid. \n",
    "  For MNIST, we'd need 10 output neurons, each for a single label. Activation function should be softmax.\n",
    "  For house prices (regression), we'd need a single output neuron with no activation function.\n",
    "\n",
    "8. Backpropagation is an algorithm for training MLPs. It works by applying the chain rule to all transformations of the MLP. It computes\n",
    "  the gradient of the loss function with respect to each parameter in the network and then it applies gradient descent to update the \n",
    "  parameters. Reverse mode auto-diff is a computational technique to efficiently compute the gradient of a composite function. It's\n",
    "  used as the implementation of backpropagation in tensorflow.\n",
    "\n",
    "9. Hyperparameters available for MLPs:\n",
    "  - Number of hidden layers\n",
    "  - Number of neurons in each hidden layer\n",
    "  - Activation functions \n",
    "  - Learning rate\n",
    "  - Batch size\n",
    "  - Optimizer\n",
    "  - Number of epochs\n",
    "  - If an ANN overfits, we should decrease:\n",
    "    * Number of hidden layers and neurons\n",
    "    * Reduce number of epochs or use early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train_full, X_test = X_train_full / 255, X_test/ 255\n",
    "\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 32s]\n",
      "val_accuracy: 0.9814000129699707\n",
      "\n",
      "Best val_accuracy So Far: 0.9832000136375427\n",
      "Total elapsed time: 00h 04m 41s\n"
     ]
    }
   ],
   "source": [
    "# We write a function that builds and compiles a model using dedicated keras objects for ranges\n",
    "\n",
    "def build_model(hp: kt.HyperParameters):\n",
    "  n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "  n_neurons = hp.Int(\"n_neurons\", min_value=50, max_value=500)\n",
    "  learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "  optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "\n",
    "  if optimizer == \"sgd\":\n",
    "    optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)\n",
    "  else:\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  for _ in range(n_hidden):\n",
    "    model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "  return model\n",
    "\n",
    "def get_run_logdir(root_logdir=\"tensorboard_logs\"):\n",
    "  return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "# example: tensorboard_logs/run_2023_10_06_08_31_16\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "bayesian_opt_tuner = kt.BayesianOptimization(\n",
    "  build_model, objective=\"val_accuracy\", seed=42, max_trials=10, alpha=1e-4, beta=2.6, overwrite=True,\n",
    "  directory=\"my_mnist\", project_name=\"bayesian_opt\"\n",
    ")\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir, profile_batch=(100,200))\n",
    "bayesian_opt_tuner.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_hidden': 7,\n",
       " 'n_neurons': 253,\n",
       " 'learning_rate': 0.0005509513888645584,\n",
       " 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best model's parameters:\n",
    "top3_params = bayesian_opt_tuner.get_best_hyperparameters(num_trials=3)\n",
    "top3_params[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 14:05:46.832869: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-10-06 14:05:46.832881: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2023-10-06 14:05:46.832894: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 163/1688 [=>............................] - ETA: 3s - loss: 0.8101 - accuracy: 0.7387"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 14:05:47.381311: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2023-10-06 14:05:47.381323: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 253/1688 [===>..........................] - ETA: 4s - loss: 0.6404 - accuracy: 0.7966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 14:05:47.701375: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2023-10-06 14:05:47.714753: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2023-10-06 14:05:47.715009: I tensorflow/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: tensorboard_logs/best_model/plugins/profile/2023_10_06_14_05_47/Adams-MacBook-Pro.local.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.2638 - accuracy: 0.9191 - val_loss: 0.1248 - val_accuracy: 0.9638\n",
      "Epoch 2/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1156 - accuracy: 0.9658 - val_loss: 0.1082 - val_accuracy: 0.9683\n",
      "Epoch 3/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0875 - accuracy: 0.9748 - val_loss: 0.0733 - val_accuracy: 0.9775\n",
      "Epoch 4/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0673 - accuracy: 0.9804 - val_loss: 0.0885 - val_accuracy: 0.9752\n",
      "Epoch 5/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.0970 - val_accuracy: 0.9728\n",
      "Epoch 6/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0461 - accuracy: 0.9867 - val_loss: 0.1270 - val_accuracy: 0.9678\n",
      "Epoch 7/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0417 - accuracy: 0.9889 - val_loss: 0.0802 - val_accuracy: 0.9803\n",
      "Epoch 8/50\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0363 - accuracy: 0.9896 - val_loss: 0.0855 - val_accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "# Create a model using these parameters and train over entire data set for longer\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "\n",
    "        tf.keras.layers.Dense(253, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(253, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(253, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(253, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(253, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(253, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(253, activation=\"relu\"),\n",
    "\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\"tensorboard_logs/best_model\", profile_batch=(100,200))\n",
    "\n",
    "history = model.fit(X_train_full, y_train_full, epochs=50, validation_split=0.1, callbacks=[early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08346851915121078, 0.9768999814987183]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
