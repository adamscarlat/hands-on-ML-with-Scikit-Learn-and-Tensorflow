{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  target\n",
       "0        0       0\n",
       "1        1       0\n",
       "2        2       0\n",
       "3        3       0\n",
       "4        4       0\n",
       "..     ...     ...\n",
       "145    145       2\n",
       "146    146       2\n",
       "147    147       2\n",
       "148    148       2\n",
       "149    149       2\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X.reset_index()\n",
    "y.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding bias term\n",
    "X_with_bias = np.c_[np.ones(len(X)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X_with_bias, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the training instances\n",
    "mean = X_train[:, 1:].mean(axis=0)\n",
    "std = X_train[:, 1:].std(axis=0)\n",
    "\n",
    "X_train[:, 1:] = (X_train[:, 1:] - mean) / std\n",
    "X_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\n",
    "X_test[:, 1:] = (X_test[:, 1:] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.diag - returns a square matrix with 1 in its diagonal.\n",
    "# y is a vector with unique labels [0,1,2]. We use them as indices into the diagonal\n",
    "# matrix. For example diag[2] will return the third row of the diag matrix which is\n",
    "# equal to [0,0,1]\n",
    "def to_one_hot(y):\n",
    "    return np.diag(np.ones(y.max() + 1))[y]\n",
    "\n",
    "y_train_one_hot = to_one_hot(y_train)\n",
    "y_valid_one_hot = to_one_hot(y_valid)\n",
    "y_test_one_hot = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = exps.sum(axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5170985980346388\n",
      "1000 0.2435495820970888\n",
      "2000 0.29587306963754856\n",
      "3000 0.34068831470097166\n",
      "4000 0.3796862602530022\n",
      "5000 0.41374130663670056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.53512166,   9.60920184, -10.19997475],\n",
       "       [ -4.41739406,   3.83429151,  -0.44632338],\n",
       "       [  5.04185003,  -0.24836876,  -6.54605345],\n",
       "       [ -6.77414938,  -7.27385862,  13.09728191],\n",
       "       [ -5.96855746,  -1.22767668,   9.34836951]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_n = 5001\n",
    "m = len(X_train)\n",
    "num_classes = len(y_train.unique())\n",
    "batch_size = 20\n",
    "step = 0.1\n",
    "epsilon = 1e-5\n",
    "theta = np.random.randn(X_train.shape[1], num_classes)\n",
    "\n",
    "for epoch in range(epochs_n):\n",
    "\n",
    "  # compute cost over validation set\n",
    "  if epoch % 1000 == 0:\n",
    "    logits_valid = X_valid @ theta\n",
    "    y_probas_valid = softmax(logits_valid)\n",
    "    xenentropy_losses = -(y_valid_one_hot * np.log(y_probas_valid + epsilon))\n",
    "    print (epoch, xenentropy_losses.sum(axis=1).mean())\n",
    "\n",
    "  for iteration in range(m):\n",
    "    indices = np.random.permutation(m)[:batch_size]    \n",
    "    X_batch = X_train[indices]\n",
    "    y_batch = y_train_one_hot[indices, :]\n",
    "\n",
    "    # forward pass\n",
    "    logits = X_batch @ theta\n",
    "    y_probas = softmax(logits)\n",
    "\n",
    "    # gradient softmax\n",
    "    error = y_probas - y_batch\n",
    "    grad = (1/m) * X_batch.T @ error\n",
    "\n",
    "    theta = theta - step * grad\n",
    "\n",
    "theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid @ theta\n",
    "probas = softmax(logits)\n",
    "predictions = probas.argmax(axis=1)\n",
    "\n",
    "accuracy_score = (predictions == y_valid).mean()\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1511\n",
      "1000 0.3868\n",
      "2000 0.3839\n",
      "3000 0.383\n",
      "4000 0.3831\n",
      "5000 0.3831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.65338463,  0.40931179, -0.87330157],\n",
       "       [-0.57502341,  0.25379909,  0.32122432],\n",
       "       [ 0.74273165, -0.4516422 , -0.29108945],\n",
       "       [-0.88488757, -0.04927848,  0.93416605],\n",
       "       [-0.82480092, -0.30155436,  1.12635528]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding l2 regularization\n",
    "\n",
    "epochs_n = 5001\n",
    "m = len(X_train)\n",
    "num_classes = len(y_train.unique())\n",
    "batch_size = 20\n",
    "step = 0.1\n",
    "epsilon = 1e-5\n",
    "alpha = 0.01\n",
    "theta = np.random.randn(X_train.shape[1], num_classes)\n",
    "\n",
    "for epoch in range(epochs_n):\n",
    "\n",
    "  # compute cost over validation set\n",
    "  if epoch % 1000 == 0:\n",
    "    logits_valid = X_valid @ theta\n",
    "    y_probas_valid = softmax(logits_valid)\n",
    "    xenentropy_losses = -(y_valid_one_hot * np.log(y_probas_valid + epsilon))\n",
    "    # not regularizing the bias parameter in theta\n",
    "    l2_loss = 1/2 * (theta[1:] ** 2 ).sum()\n",
    "    total_loss = xenentropy_losses.sum(axis=1).mean() + alpha * l2_loss\n",
    "    print (epoch, total_loss.round(4))\n",
    "\n",
    "  for iteration in range(m):\n",
    "    indices = np.random.permutation(m)[:batch_size]    \n",
    "    X_batch = X_train[indices]\n",
    "    y_batch = y_train_one_hot[indices, :]\n",
    "\n",
    "    # forward pass\n",
    "    logits = X_batch @ theta\n",
    "    y_probas = softmax(logits)\n",
    "\n",
    "    # gradient softmax\n",
    "    error = y_probas - y_batch\n",
    "    grad = (1/m) * X_batch.T @ error\n",
    "    grad += np.r_[np.zeros([1, num_classes]), alpha * theta[1:]]\n",
    "\n",
    "    theta = theta - step * grad\n",
    "\n",
    "theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid @ theta\n",
    "probas = softmax(logits)\n",
    "predictions = probas.argmax(axis=1)\n",
    "\n",
    "accuracy_score = (predictions == y_valid).mean()\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_test @ theta\n",
    "probas = softmax(logits)\n",
    "predictions = probas.argmax(axis=1)\n",
    "\n",
    "accuracy_score = (predictions == y_test).mean()\n",
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
