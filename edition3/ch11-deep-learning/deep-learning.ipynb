{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 784)               3136      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "  \n",
    "  # BN layer as a first, input normalization layer\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "  # BN layer after each hidden layer\n",
    "  tf.keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "  tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "  tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "For this example, we'll use fashion MNIST. We'll assume that someone trained a model for 8/10 classes of the dataset (all except sandal\n",
    "and T-shirt). We'll call this model A. \n",
    "\n",
    "The problem you're trying to solve is classifying T-Shirts (positive) from sandals (negative). You use model A as your base model for\n",
    "transfer learning.\n",
    "\n",
    "**Warning!**\n",
    "\n",
    "This example is hella contrived. In reality, transfer learning does not work well on small NN as it these networks learn very\n",
    "specific patterns to the data that are not very reusable. In practice, you'd want to use transfer learning only on DNNs that \n",
    "are complex and deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1376/1376 [==============================] - 1s 705us/step - loss: 1.1496 - accuracy: 0.6601 - val_loss: 0.6729 - val_accuracy: 0.7894\n",
      "Epoch 2/20\n",
      "1376/1376 [==============================] - 1s 636us/step - loss: 0.5717 - accuracy: 0.8172 - val_loss: 0.4979 - val_accuracy: 0.8315\n",
      "Epoch 3/20\n",
      "1376/1376 [==============================] - 1s 664us/step - loss: 0.4613 - accuracy: 0.8497 - val_loss: 0.4317 - val_accuracy: 0.8516\n",
      "Epoch 4/20\n",
      "1376/1376 [==============================] - 1s 649us/step - loss: 0.4091 - accuracy: 0.8645 - val_loss: 0.3936 - val_accuracy: 0.8649\n",
      "Epoch 5/20\n",
      "1376/1376 [==============================] - 1s 658us/step - loss: 0.3770 - accuracy: 0.8741 - val_loss: 0.3704 - val_accuracy: 0.8721\n",
      "Epoch 6/20\n",
      "1376/1376 [==============================] - 1s 649us/step - loss: 0.3548 - accuracy: 0.8805 - val_loss: 0.3525 - val_accuracy: 0.8754\n",
      "Epoch 7/20\n",
      "1376/1376 [==============================] - 1s 649us/step - loss: 0.3384 - accuracy: 0.8859 - val_loss: 0.3388 - val_accuracy: 0.8814\n",
      "Epoch 8/20\n",
      "1376/1376 [==============================] - 1s 662us/step - loss: 0.3251 - accuracy: 0.8886 - val_loss: 0.3368 - val_accuracy: 0.8844\n",
      "Epoch 9/20\n",
      "1376/1376 [==============================] - 1s 644us/step - loss: 0.3148 - accuracy: 0.8928 - val_loss: 0.3199 - val_accuracy: 0.8874\n",
      "Epoch 10/20\n",
      "1376/1376 [==============================] - 1s 677us/step - loss: 0.3055 - accuracy: 0.8953 - val_loss: 0.3170 - val_accuracy: 0.8897\n",
      "Epoch 11/20\n",
      "1376/1376 [==============================] - 1s 640us/step - loss: 0.2977 - accuracy: 0.8973 - val_loss: 0.3078 - val_accuracy: 0.8920\n",
      "Epoch 12/20\n",
      "1376/1376 [==============================] - 1s 668us/step - loss: 0.2910 - accuracy: 0.9000 - val_loss: 0.3083 - val_accuracy: 0.8910\n",
      "Epoch 13/20\n",
      "1376/1376 [==============================] - 1s 722us/step - loss: 0.2857 - accuracy: 0.9018 - val_loss: 0.2980 - val_accuracy: 0.8975\n",
      "Epoch 14/20\n",
      "1376/1376 [==============================] - 1s 677us/step - loss: 0.2800 - accuracy: 0.9028 - val_loss: 0.2923 - val_accuracy: 0.8992\n",
      "Epoch 15/20\n",
      "1376/1376 [==============================] - 1s 642us/step - loss: 0.2756 - accuracy: 0.9047 - val_loss: 0.2879 - val_accuracy: 0.9017\n",
      "Epoch 16/20\n",
      "1376/1376 [==============================] - 1s 721us/step - loss: 0.2709 - accuracy: 0.9063 - val_loss: 0.2852 - val_accuracy: 0.9015\n",
      "Epoch 17/20\n",
      "1376/1376 [==============================] - 1s 650us/step - loss: 0.2670 - accuracy: 0.9066 - val_loss: 0.2802 - val_accuracy: 0.9042\n",
      "Epoch 18/20\n",
      "1376/1376 [==============================] - 1s 659us/step - loss: 0.2628 - accuracy: 0.9094 - val_loss: 0.2798 - val_accuracy: 0.9050\n",
      "Epoch 19/20\n",
      "1376/1376 [==============================] - 1s 635us/step - loss: 0.2593 - accuracy: 0.9105 - val_loss: 0.2764 - val_accuracy: 0.9070\n",
      "Epoch 20/20\n",
      "1376/1376 [==============================] - 1s 678us/step - loss: 0.2558 - accuracy: 0.9113 - val_loss: 0.2758 - val_accuracy: 0.9045\n",
      "INFO:tensorflow:Assets written to: my_model_A/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_A/assets\n"
     ]
    }
   ],
   "source": [
    "# Training model A - the model we'll use as the base for the transfer learning. In reality, this model will be from a library of \n",
    "# pretrained models.\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "pos_class_id = class_names.index(\"Pullover\")\n",
    "neg_class_id = class_names.index(\"T-shirt/top\")\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
    "    y_A = y[~y_for_B]\n",
    "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
    "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
    "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
    "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
    "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_A = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(\"my_model_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing model B - the model that will use model A as its base for transfer learning\n",
    "\n",
    "# Make a clone of model A (to not make changes to the actual model A)\n",
    "model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "\n",
    "# Cloning a model does not clone the weights, just the architecture. We need to add the weights\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "# Getting all of model A's layers except the output layer\n",
    "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
    "\n",
    "# Adding a new output layer for binary classification\n",
    "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Freeze all layers (make them non-trainable) except the new layer\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Compile Model B\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.5713 - accuracy: 0.7300 - val_loss: 0.5714 - val_accuracy: 0.7527\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5479 - accuracy: 0.8100 - val_loss: 0.5589 - val_accuracy: 0.7972\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5339 - accuracy: 0.8500 - val_loss: 0.5547 - val_accuracy: 0.7992\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5221 - accuracy: 0.8700 - val_loss: 0.5460 - val_accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "# First we run a few epochs with every layer frozen except the output layer. This will ensure that the output layer's\n",
    "# weights are not totally random (before we unfreeze the lower layers). If we unfreeze the lower layers (where the weights\n",
    "# are tuned), the large gradients from the random weights will wreck the fine tuning.\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2464 - accuracy: 0.9550 - val_loss: 0.2874 - val_accuracy: 0.9407\n",
      "Epoch 2/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2403 - accuracy: 0.9550 - val_loss: 0.2801 - val_accuracy: 0.9397\n",
      "Epoch 3/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2328 - accuracy: 0.9550 - val_loss: 0.2739 - val_accuracy: 0.9387\n",
      "Epoch 4/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2267 - accuracy: 0.9550 - val_loss: 0.2686 - val_accuracy: 0.9397\n",
      "Epoch 5/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2212 - accuracy: 0.9550 - val_loss: 0.2634 - val_accuracy: 0.9416\n",
      "Epoch 6/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9550 - val_loss: 0.2587 - val_accuracy: 0.9407\n",
      "Epoch 7/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9600 - val_loss: 0.2569 - val_accuracy: 0.9397\n",
      "Epoch 8/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2095 - accuracy: 0.9550 - val_loss: 0.2508 - val_accuracy: 0.9387\n",
      "Epoch 9/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9550 - val_loss: 0.2467 - val_accuracy: 0.9387\n",
      "Epoch 10/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1991 - accuracy: 0.9550 - val_loss: 0.2424 - val_accuracy: 0.9416\n",
      "Epoch 11/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1955 - accuracy: 0.9550 - val_loss: 0.2389 - val_accuracy: 0.9416\n",
      "Epoch 12/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1919 - accuracy: 0.9600 - val_loss: 0.2356 - val_accuracy: 0.9426\n",
      "Epoch 13/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9550 - val_loss: 0.2325 - val_accuracy: 0.9436\n",
      "Epoch 14/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.9600 - val_loss: 0.2297 - val_accuracy: 0.9456\n",
      "Epoch 15/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1827 - accuracy: 0.9650 - val_loss: 0.2269 - val_accuracy: 0.9466\n",
      "Epoch 16/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9600 - val_loss: 0.2243 - val_accuracy: 0.9466\n",
      "Epoch 17/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9600 - val_loss: 0.2214 - val_accuracy: 0.9456\n",
      "Epoch 18/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9550 - val_loss: 0.2190 - val_accuracy: 0.9456\n",
      "Epoch 19/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1713 - accuracy: 0.9600 - val_loss: 0.2166 - val_accuracy: 0.9456\n",
      "Epoch 20/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1695 - accuracy: 0.9600 - val_loss: 0.2176 - val_accuracy: 0.9486\n",
      "Epoch 21/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9600 - val_loss: 0.2141 - val_accuracy: 0.9466\n",
      "Epoch 22/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1658 - accuracy: 0.9600 - val_loss: 0.2111 - val_accuracy: 0.9456\n",
      "Epoch 23/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1634 - accuracy: 0.9650 - val_loss: 0.2092 - val_accuracy: 0.9456\n",
      "Epoch 24/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1615 - accuracy: 0.9600 - val_loss: 0.2073 - val_accuracy: 0.9456\n",
      "Epoch 25/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1591 - accuracy: 0.9600 - val_loss: 0.2058 - val_accuracy: 0.9456\n",
      "Epoch 26/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9650 - val_loss: 0.2039 - val_accuracy: 0.9456\n",
      "Epoch 27/32\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9600 - val_loss: 0.2045 - val_accuracy: 0.9436\n",
      "Epoch 28/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1548 - accuracy: 0.9600 - val_loss: 0.2034 - val_accuracy: 0.9436\n",
      "Epoch 29/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1533 - accuracy: 0.9600 - val_loss: 0.1998 - val_accuracy: 0.9476\n",
      "Epoch 30/32\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1511 - accuracy: 0.9650 - val_loss: 0.1981 - val_accuracy: 0.9466\n",
      "Epoch 31/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9650 - val_loss: 0.1974 - val_accuracy: 0.9476\n",
      "Epoch 32/32\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9650 - val_loss: 0.1961 - val_accuracy: 0.9476\n"
     ]
    }
   ],
   "source": [
    "# Now the weights of the output layers are not totally random, we can unfreeze the lower layers and proceed with the\n",
    "# training\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "  layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=32, validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 529us/step - loss: 0.1990 - accuracy: 0.9440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1989995390176773, 0.9440000057220459]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power scheduling\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.01, decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3082 - accuracy: 0.8902 - lr: 0.0100\n",
      "Epoch 2/3\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2968 - accuracy: 0.8930 - lr: 0.0089\n",
      "Epoch 3/3\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2853 - accuracy: 0.8967 - lr: 0.0079\n"
     ]
    }
   ],
   "source": [
    "# Exponential scheduling\n",
    "def exponential_decay(lr0, s):\n",
    "  def exponential_decay_fn(epoch):\n",
    "    return lr0 * 0.1 ** (epoch / s)\n",
    "  return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "# This callback updates the optimizer's learning_rate attribute at the beginning of each epoch\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=3, callbacks=[lr_schedule])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
