{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "import email\n",
    "import email.policy\n",
    "import re\n",
    "import nltk\n",
    "import urlextract\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from html import unescape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets/spam/ham.tar.bz2\n",
      "Downloading datasets/spam/spam.tar.bz2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fetch_spam_data():\n",
    "    spam_root = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "    ham_url = spam_root + \"20030228_easy_ham.tar.bz2\"\n",
    "    spam_url = spam_root + \"20030228_spam.tar.bz2\"\n",
    "\n",
    "    spam_path = Path() / \"datasets\" / \"spam\"\n",
    "    spam_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for dir_name, tar_name, url in ((\"easy_ham\", \"ham\", ham_url), (\"spam\", \"spam\", spam_url)):\n",
    "        if not (spam_path / dir_name).is_dir():\n",
    "            path = (spam_path / tar_name).with_suffix(\".tar.bz2\")\n",
    "            print(\"Downloading\", path)\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            tar_bz2_file = tarfile.open(path)\n",
    "            tar_bz2_file.extractall(path=spam_path)\n",
    "            tar_bz2_file.close()\n",
    "\n",
    "    return [spam_path / dir_name for dir_name in (\"easy_ham\", \"spam\")]\n",
    "\n",
    "ham_dir, spam_dir = fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load email file names\n",
    "ham_filenames = [f for f in sorted(ham_dir.iterdir()) if len(f.name) > 20]\n",
    "spam_filenames = [f for f in sorted(spam_dir.iterdir()) if len(f.name) > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham count:  2500\n",
      "Spam count:  500\n"
     ]
    }
   ],
   "source": [
    "print (\"Ham count: \", len(ham_filenames))\n",
    "print (\"Spam count: \", len(spam_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham count:  2500\n",
      "Spam count:  500\n"
     ]
    }
   ],
   "source": [
    "def load_email(filepath):\n",
    "  with open(filepath, \"rb\") as f:\n",
    "    return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "  \n",
    "ham_emails = [load_email(filepath) for filepath in ham_filenames]\n",
    "spam_emails = [load_email(filepath) for filepath in spam_filenames]\n",
    "\n",
    "# double checking that the count is the same as above\n",
    "print (\"Ham count: \", len(ham_emails))\n",
    "print (\"Spam count: \", len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I just had to jump in here as Carbonara is one of my favourites to make and \n",
      "> ask \n",
      "> what the hell are you supposed to use instead of cream? \n",
      "\n",
      "Isn't it just basically a mixture of beaten egg and bacon (or pancetta, \n",
      "really)? You mix in the raw egg to the cooked pasta and the heat of the pasta \n",
      "cooks the egg. That's my understanding.\n",
      "\n",
      "Martin\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print (ham_emails[5].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A POWERHOUSE GIFTING PROGRAM You Don't Want To Miss! \n",
      " \n",
      "  GET IN WITH THE FOUNDERS! \n",
      "The MAJOR PLAYERS are on This ONE\n",
      "For ONCE be where the PlayerS are\n",
      "This is YOUR Private Invitation\n",
      "\n",
      "EXPERTS ARE CALLING THIS THE FASTEST WAY \n",
      "TO HUGE CASH FLOW EVER CONCEIVED\n",
      "Leverage $1,000 into $50,000 Over and Over Again\n",
      "\n",
      "THE QUESTION HERE IS:\n",
      "YOU EITHER WANT TO BE WEALTHY \n",
      "OR YOU DON'T!!!\n",
      "WHICH ONE ARE YOU?\n",
      "I am tossing you a financial lifeline and for your sake I \n",
      "Hope you GRAB onto it and hold on tight For the Ride of youR life!\n",
      "\n",
      "Testimonials\n",
      "\n",
      "Hear what average people are doing their first few days:\n",
      "�We've received 8,000 in 1 day and we are doing that over and over again!' Q.S. in AL\n",
      " �I'm a single mother in FL and I've received 12,000 in the last 4 days.� D. S. in FL\n",
      "�I was not sure about this when I sent off my $1,000 pledge, but I got back $2,000 the very next day!� L.L. in KY\n",
      "�I didn't have the money, so I found myself a partner to work this with. We have received $4,000 over the last 2 days. \n",
      "I think I made the right decision; don't you?� K. C. in FL\n",
      "�I pick up $3,000 my first day and I  they gave me free leads and all the training, you can too!� J.W. in CA\n",
      "\n",
      "ANNOUNCING: We will CLOSE your sales for YOU! And Help you get a Fax Blast IMMEDIATELY Upon Your Entry!!!    YOU Make the MONEY!!!\n",
      "FREE LEADS!!! TRAINING!!!\n",
      "\n",
      "$$DON'T WAIT!!! CALL NOW $$\n",
      "FAX BACK TO: 1-800-421-6318 OR Call 1-800-896-6568 \n",
      "\n",
      "Name__________________________________Phone___________________________________________\n",
      "\n",
      "Fax_____________________________________Email____________________________________________\n",
      "\n",
      "Best Time To Call_________________________Time Zone________________________________________\n",
      "\n",
      "This message is sent in compliance of the new e-mail bill. \"Per Section 301, Paragraph (a)(2)(C) of S. 1618, further transmissions by the sender of this email may be stopped, at no cost to you, by sending a reply to this email address with the word \"REMOVE\" in the subject line. Errors, omissions, and exceptions excluded.\n",
      " \n",
      "This is NOT spam! I have compiled this list from our Replicate Database, relative to Seattle Marketing Group, The Gigt, or Turbo Team for the sole purpose of these communications. Your continued inclusion is ONLY by your gracious permission. If you wish to not receive this mail from me, please send an email to tesrewinter@yahoo.com with \"Remove\" in the subject and you will be deleted immediately.\n"
     ]
    }
   ],
   "source": [
    "print (spam_emails[5].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham emails most common structures:\n",
      "[('text/plain', 2408),\n",
      " ('multipart(text/plain, application/pgp-signature)', 66),\n",
      " ('multipart(text/plain, text/html)', 8),\n",
      " ('multipart(text/plain, text/plain)', 4),\n",
      " ('multipart(text/plain)', 3)]\n",
      "------------------------------------------------------------\n",
      "Spam emails most common structures:\n",
      "[('text/plain', 218),\n",
      " ('text/html', 183),\n",
      " ('multipart(text/plain, text/html)', 45),\n",
      " ('multipart(text/html)', 20),\n",
      " ('multipart(text/plain)', 19)]\n"
     ]
    }
   ],
   "source": [
    "# Exploring email structures\n",
    "\n",
    "def get_email_structure(email):\n",
    "  # text/plain email type\n",
    "  if isinstance(email, str):\n",
    "    return email\n",
    "  \n",
    "  payload = email.get_payload()\n",
    "  # multipart email type (with types embedded)\n",
    "  if isinstance(payload, list):\n",
    "    # attachments can have emails inside of them\n",
    "    multipart = \", \".join([get_email_structure(sub_email) for sub_email in payload])\n",
    "    \n",
    "    return f\"multipart({multipart})\"\n",
    "\n",
    "  # other email type\n",
    "  else:\n",
    "    return email.get_content_type()\n",
    "  \n",
    "def structures_counter(emails):\n",
    "  structures = Counter()\n",
    "  for email in emails:\n",
    "    structure = get_email_structure(email)\n",
    "    structures[structure] += 1\n",
    "  \n",
    "  return structures\n",
    "\n",
    "print (\"Ham emails most common structures:\")\n",
    "pprint (structures_counter(ham_emails).most_common()[:5])\n",
    "\n",
    "print (\"-\"*60)\n",
    "\n",
    "print (\"Spam emails most common structures:\")\n",
    "pprint (structures_counter(spam_emails).most_common()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the email structure has useful information in it that can help classification. The ham emails are mostly text plain\n",
    "and some have pgp signatures while no spam emails have pgp signatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "# Exploring email headers\n",
    "\n",
    "for header, value in spam_emails[0].items():\n",
    "  print (header, \":\", value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train / test split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse HTML in emails\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "  # remove head section\n",
    "  text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "\n",
    "  # replace all <a>...</a> with HYPERLINK\n",
    "  text = re.sub('<a\\s.*?>', 'HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "\n",
    "  # remove all HTML tags\n",
    "  text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "\n",
    "  # remove white spaces\n",
    "  text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "\n",
    "  # unescape html (e.g &gt or &nbsp)\n",
    "  return unescape(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE html_to_plain_text: \n",
      "<HTML><HEAD><TITLE></TITLE><META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"><STYLE>A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}</STYLE><META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"></HEAD>\n",
      "<BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"><TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"><TR><TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\">\n",
      "<font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\">\n",
      "<b>OTC</b></font></TD></TR><TR><TD height=\"2\" bgcolor=\"#6a694f\">\n",
      "<font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\">\n",
      "<b>&nbsp;Newsletter</b></font></TD><TD height=\"2\" bgcolor=\"#6a694f\"><div align=\"right\"><font color=\"#FFFFFF\">\n",
      "<b>Discover Tomorrow's Winners&nbsp;</b></font></div></TD></TR><TR><TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"><table width=\"100%\" border=\"0\"  ...\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "AFTER html_to_plain_text: \n",
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
      "Put CBYI on your watch list, acquire a position TODAY.\n",
      "REASONS TO INVEST IN CBYI\n",
      "A profitable company and is on track to beat ALL earnings estimates!\n",
      "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
      "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
      "RAPIDLY GROWING INDUSTRY\n",
      "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ...\n"
     ]
    }
   ],
   "source": [
    "# Getting all html type emails (only spam category had any)\n",
    "html_spam_emails = [email for email in X_train[y_train == 1] if get_email_structure(email) == \"text/html\"]\n",
    "\n",
    "# Seeing one of them before and after the html_to_plain_text function\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print (\"BEFORE html_to_plain_text: \")\n",
    "print (sample_html_spam.get_content().strip()[:1000], \"...\")\n",
    "\n",
    "print (\"-\" * 200)\n",
    "\n",
    "print (\"AFTER html_to_plain_text: \")\n",
    "print (html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Wat ...\n"
     ]
    }
   ],
   "source": [
    "# Converting an email object to plain text (emails are python objects)\n",
    "\n",
    "def email_to_text(email):\n",
    "  html = None\n",
    "  for part in email.walk():\n",
    "    ctype = part.get_content_type()\n",
    "    \n",
    "    # email types we're not handling\n",
    "    if not ctype in (\"text/plain\", \"text/html\"):\n",
    "      continue\n",
    "\n",
    "    try:\n",
    "      content = part.get_content()\n",
    "    except:\n",
    "      # in case of encoding issues\n",
    "      content = str(part.get_payload())\n",
    "    \n",
    "    if ctype == \"text/plain\":\n",
    "      return content\n",
    "    else:\n",
    "      html = content\n",
    "  \n",
    "  if html:\n",
    "    return html_to_plain_text(html)\n",
    "\n",
    "print (email_to_text(sample_html_spam)[:100], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the text \n",
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['github.com', 'https://kdjalsdjk.com']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting URLs from the text\n",
    "url_extractor = urlextract.URLExtract()\n",
    "\n",
    "# example usage\n",
    "some_text = \"Will it detect github.com and https://kdjalsdjk.com?\"\n",
    "url_extractor.find_urls(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting together all our transformations above into a transformer. It cleans the emails and parses them into text\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "               replace_urls=True, replace_numbers=True, stemming=True) -> None:\n",
    "      self.strip_headers = strip_headers\n",
    "      self.lower_case = lower_case\n",
    "      self.remove_punctuation = remove_punctuation\n",
    "      self.replace_urls = replace_urls\n",
    "      self.replace_numbers = replace_numbers\n",
    "      self.stemming = stemming\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "     return self\n",
    "  \n",
    "  def transform(self, X, y=None):\n",
    "     X_transformed = []\n",
    "     for email in X:\n",
    "        # get the email's text\n",
    "        text = email_to_text(email)\n",
    "        \n",
    "        if not text: \n",
    "           text = \" \"\n",
    "\n",
    "        # lower case text\n",
    "        if self.lower_case:\n",
    "           text = text.lower()\n",
    "        \n",
    "        # replace URLs with the word URL\n",
    "        if self.replace_urls and url_extractor is not None:\n",
    "           urls = list(set(url_extractor.find_urls(text)))\n",
    "           for url in urls:\n",
    "              text = text.replace(url, \" URL \")\n",
    "        \n",
    "        # replace numbers with the word NUMBER\n",
    "        if self.replace_numbers:\n",
    "           text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "\n",
    "        word_counts = Counter(text.split())\n",
    "\n",
    "        if self.stemming and stemmer is not None:\n",
    "           stemmed_word_counts = Counter()\n",
    "           for word, count in word_counts.items():\n",
    "              stemmed_word = stemmer.stem(word)\n",
    "              stemmed_word_counts[stemmed_word] += count\n",
    "           word_counts = stemmed_word_counts\n",
    "        \n",
    "        X_transformed.append(word_counts)\n",
    "     \n",
    "     return np.array(X_transformed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'chuck': 1, 'murcko': 1, 'wrote:': 1, '>[...stuff...]': 1, 'yawn.': 1, 'r': 1})\n",
      " Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'to': 3, 'by': 3, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'half': 2, 'teach': 2, 'some': 1, 'interest': 1, 'quotes...': 1, 'url': 1, 'thoma': 1, 'jefferson:': 1, '\"i': 1, 'examin': 1, 'known': 1, 'word,': 1, 'i': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'christian': 1, 'redeem': 1, 'feature.': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mythology.': 1, 'million': 1, 'innoc': 1, 'men,': 1, 'women': 1, 'children,': 1, 'sinc': 1, 'introduct': 1, 'christianity,': 1, 'burnt,': 1, 'tortured,': 1, 'fine': 1, 'imprisoned.': 1, 'what': 1, 'ha': 1, 'effect': 1, 'thi': 1, 'coercion?': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrites;': 1, 'support': 1, 'rogueri': 1, 'error': 1, 'over': 1, 'earth.\"': 1, 'six': 1, 'histor': 1, 'americans,': 1, 'john': 1, 'e.': 1, 'remsburg,': 1, 'letter': 1, 'william': 1, 'short': 1, 'jefferson': 1, 'again:': 1, '\"christianity...(ha': 1, 'become)': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man.': 1, '...rogueries,': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'jesu': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul,': 1, 'first': 1, 'great': 1, 'corrupt': 1, 'jesus.\"': 1})\n",
      " Counter({'>': 5, 'url': 4, 'in': 2, 'an': 2, 'and': 2, 'yahoo!': 2, 'group': 2, 'to': 2, '---': 1, 'forteana@y...,': 1, '\"martin': 1, 'adamson\"': 1, '<martin@s...>': 1, 'wrote:': 1, 'for': 1, 'alternative,': 1, 'rather': 1, 'more': 1, 'factual': 1, 'based,': 1, 'rundown': 1, 'on': 1, \"hamza'\": 1, 'career,': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright:': 1, 'we': 1, 'know': 1, 'how': 1, 'unbias': 1, 'memri': 1, 'is,': 1, \"don't\": 1, 'we....': 1, 'html': 1, 'rob': 1, '------------------------': 1, 'sponsor': 1, '---------------------~-->': 1, 'number': 1, 'dvd': 1, 'free': 1, '+s&p': 1, 'join': 1, 'now': 1, '---------------------------------------------------------------------~->': 1, 'unsubscrib': 1, 'from': 1, 'thi': 1, 'group,': 1, 'send': 1, 'email': 1, 'to:': 1, 'forteana-unsubscribe@egroups.com': 1, 'your': 1, 'use': 1, 'of': 1, 'is': 1, 'subject': 1})]\n"
     ]
    }
   ],
   "source": [
    "# trying out the transformer on some email examples\n",
    "\n",
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "print (X_few_wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting word counts into a matrix: \n",
    "# Each row represents an email's word count array\n",
    "# Each column will indicate the count/absence of a word \n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, vocabulary_size=1000) -> None:\n",
    "    self.vocabulary_size = vocabulary_size\n",
    "  \n",
    "  # Builds a vocabulary of the <vocabulary_size> most common words\n",
    "  def fit(self, X, y=None):\n",
    "    total_count = Counter()\n",
    "    for word_count in X:\n",
    "      for word, count in word_count.items():\n",
    "        # capping counts at 10\n",
    "        total_count[word] += min(count, 10)\n",
    "    most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "    self.vocabulary_ = {word: index+1 for index, (word, count) in enumerate(most_common)}\n",
    "\n",
    "    return self\n",
    "\n",
    "  # Builds a sparse matrix where each row is an email example and each column \n",
    "  # indicates the count/absence of a word \n",
    "  def transform(self, X, y=None):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    # for each email (row), get the word_count Counter\n",
    "    for row, word_count in enumerate(X):\n",
    "      # for each word in the Counter, get the word's index in the vocab and append the \n",
    "      # count of the word\n",
    "      for word, count in word_count.items():\n",
    "        rows.append(row)\n",
    "        cols.append(self.vocabulary_.get(word, 0))\n",
    "        data.append(count)\n",
    "    \n",
    "    return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6   0   0   0   0   0   0   0   0   0   0]\n",
      " [101  11   9   8   1   3   0   3   1   2   3]\n",
      " [ 64   0   1   2   4   2   5   1   2   1   0]]\n",
      "{'the': 1, 'of': 2, 'and': 3, 'url': 4, 'to': 5, '>': 6, 'all': 7, 'in': 8, 'on': 9, 'by': 10}\n"
     ]
    }
   ],
   "source": [
    "# Testing the WordCounterToVectorTransformer on a few examples\n",
    "\n",
    "# The first column in the matrix represents the 0th index which means words not found in the most common word vocab.\n",
    "# Each column after represents a word and its count\n",
    "\n",
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "print (X_few_vectors.toarray())\n",
    "print (vocab_transformer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline from both transformers\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "  (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "  (\"wordcount_to_vector\", WordCounterToVectorTransformer())\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9574468085106383\n",
      "Recall:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print (f\"Precision: \", precision_score(y_test, y_pred))\n",
    "print (f\"Recall: \", recall_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples = [\n",
    "  [\"MUST REPLY NOW!!!\"]\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
