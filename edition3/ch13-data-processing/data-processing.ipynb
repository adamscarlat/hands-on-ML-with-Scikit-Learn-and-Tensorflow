{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tf.data API\n",
    "\n",
    "The tf.data API is a streaming API. It lets you efficiently iterate through a large dataset's records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int32)\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "tf.Tensor([5 6 7 8 9], shape=(5,), dtype=int32)\n",
      "--------------------------------------------------\n",
      "tf.Tensor([ 0  1  4  9 16], shape=(5,), dtype=int32)\n",
      "tf.Tensor([25 36 49 64 81], shape=(5,), dtype=int32)\n",
      "tf.Tensor([ 0  1  4  9 16], shape=(5,), dtype=int32)\n",
      "tf.Tensor([25 36 49 64 81], shape=(5,), dtype=int32)\n",
      "tf.Tensor([ 0  1  4  9 16], shape=(5,), dtype=int32)\n",
      "tf.Tensor([25 36 49 64 81], shape=(5,), dtype=int32)\n",
      "--------------------------------------------------\n",
      "tf.Tensor([25 36 49 64 81], shape=(5,), dtype=int32)\n",
      "tf.Tensor([25 36 49 64 81], shape=(5,), dtype=int32)\n",
      "tf.Tensor([25 36 49 64 81], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Chaining transformations\n",
    "\n",
    "# Duplicate the dataset 3 times and create batches of 5 items\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3).batch(5)\n",
    "for item in dataset:\n",
    "  print (item)\n",
    "\n",
    "print (\"-\" * 50)\n",
    "\n",
    "# Applying a lambda function to the elements. \n",
    "# Any function passed to this API will be converted to a tf.function and must follow the tf function rules\n",
    "dataset = dataset.map(lambda x: x ** 2)\n",
    "for item in dataset:\n",
    "  print (item)\n",
    "\n",
    "print (\"-\" * 50)\n",
    "\n",
    "# Filtering the data\n",
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "for item in dataset:\n",
    "  print (item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling\n",
    "\n",
    "Shuffling a tf dataset creates a buffer in memory (of a given size). Then it fills it from the dataset, shuffles it and \n",
    "outputs an item. It does that until the entire dataset is utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 0-9 repeated twice\n",
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "\n",
    "# Get shuffled batches of 7 items\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "  print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling data from multiple files\n",
    "\n",
    "# Using the California dataset to demonstrate shuffling data in multiple files.\n",
    "# Creating a train, validation, test sets.\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Assume that this dataset is too big to fit in memory. We split it into files\n",
    "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = Path() / \"datasets\" / \"housing\"\n",
    "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename_format = \"my_{}_{:02d}.csv\"\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    chunks = np.array_split(np.arange(m), n_parts)\n",
    "    for file_idx, row_indices in enumerate(chunks):\n",
    "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(str(part_csv))\n",
    "        with open(part_csv, \"w\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "# Combining data and labels before saving them to chunked files\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "\n",
    "# Add column names since they are not in the datasets\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peeking into the first chunked file's 4 lines\n",
    "print(\"\".join(open(train_filepaths[0]).readlines()[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the dataset (which in theory doesn't fit in memory) as chunked files, we\n",
    "# build the input pipeline\n",
    "\n",
    "# We create a tf dataset of filepaths (let tf manage reading from these file). tf will\n",
    "# shuffle the filepaths as well.\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "# We call interleave to read rows from multiple files at a time\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "  # Let tf decide the number of threads\n",
    "  num_parallel_calls=tf.data.AUTOTUNE,\n",
    "  # Skip the header line\n",
    "  map_func=lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782', shape=(), dtype=string)\n",
      "tf.Tensor(b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "  print (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "We now have a dataset built from the housing dataset. Each line is a tensor that contains a byte string. \n",
    "We need to preprocess the data - parse the byte string and scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the mean and std of the training data. This can be done on a large enough sample of the training \n",
    "# dataset (in this scenario the dataset does not fit in memory).\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean, X_std = scaler.mean_, scaler.scale_\n",
    "\n",
    "n_inputs = 8\n",
    "\n",
    "def parse_csv_line(line):\n",
    "  # Default array for each row. Tells tf the datatype for each column.\n",
    "  # We also add the last column(label) separately without filling in missing values (it\n",
    "  # will raise an exception instead).\n",
    "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "\n",
    "  # Returns a list of scalar tensors \n",
    "  fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "\n",
    "  # Using stack to turn these scalars into a 1D tensor for the features\n",
    "  # and another 1D tensor for the label\n",
    "  return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "# Parse and scale a line from the CSV file\n",
    "def preprocessor(line):\n",
    "  x, y = parse_csv_line(line)\n",
    "  return (x - X_mean) / X_std, y\n",
    "\n",
    "# Example - 1 line parsing\n",
    "preprocessor(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Using all the functionalities we showed above to parse a folder containing a chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocessor to every line in the dataset and getting back \n",
    "# batches of shuffled data.\n",
    "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None, \n",
    "                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42, batch_size=32):\n",
    "  \n",
    "  # Let tf manage the filepaths\n",
    "  dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "  dataset = dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers, num_parallel_calls=n_read_threads\n",
    "  )\n",
    "  \n",
    "  dataset = dataset.map(preprocessor, num_parallel_calls=n_parse_threads)\n",
    "  dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "\n",
    "  # prefetch prepares the next batch while the GPU is working on the current one.\n",
    "  # Better for performance\n",
    "  return dataset.batch(batch_size).prefetch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dataset with Keras\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.8034 - accuracy: 0.0028 - val_loss: 0.9290 - val_accuracy: 0.0044\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5661 - accuracy: 0.0026 - val_loss: 16.5037 - val_accuracy: 0.0044\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6649 - accuracy: 0.0027 - val_loss: 11.9535 - val_accuracy: 0.0044\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5525 - accuracy: 0.0029 - val_loss: 18.8439 - val_accuracy: 0.0044\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5559 - accuracy: 0.0028 - val_loss: 18.7824 - val_accuracy: 0.0044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1757a5490>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Preprocessing Layers\n",
    "\n",
    "### The normalization layer\n",
    "\n",
    "This layer standardizes the inputs. Ths layer needs an extra `adapt` call before using it with passing it the entire\n",
    "dataset. The reason is, to standardizes the data, it needs to know the mean and variance. If we just add it to the network\n",
    "without adapting it, it will only see batches which are too small to get a representative mean and variance.\n",
    "\n",
    "A few hundred samples can also be enough for getting the mean and variance.\n",
    "\n",
    "After adapting this later during training, we don't need to worry about it anymore. It stores the parameters and uses them for inference.\n",
    "Remember that standardization parameters are defined using the training set always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "model = tf.keras.models.Sequential([\n",
    "  norm_layer,\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "norm_layer.adapt(X_train)\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding preprocessing layers directly into the model is convenient since we don't have to worry about preprocessing in \n",
    "production, however, it has the potential of slowing down training. Some preprocessing has to happen once before\n",
    "training (like standardization). When we add it to the model, it runs on every batch unnecessarily.\n",
    "\n",
    "To fix this, we use these layers outside the model, run training and then create a new model that wraps these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "# Model used for training\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=5)\n",
    "\n",
    "# Model used for production\n",
    "final_model = tf.keras.models.Sequential([\n",
    "  norm_layer,\n",
    "  tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - how to write a normalization layer\n",
    "\n",
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "  def adapt(self, X):\n",
    "    self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "    self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    eps = tf.keras.backend.epsilon()\n",
    "    return (inputs - self.mean_) / (self.std_ + eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - using the Normalization and CategoryEncoding layers on a dataset\n",
    "\n",
    "Using the Normalization layer on a specific numerical feature in dataset that has different feature types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Discretization Layer\n",
    "\n",
    "Transforms numerical features into categorical features by mapping value ranges into categories (bins).\n",
    "\n",
    "This is useful for features that have a highly non-linear relationship with the label. For example, `age`.\n",
    "\n",
    "The resulting categories should get one-hot encoded before passed into the network (see next layer  CategoryEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
    "\n",
    "# This will create 3 bins (x < 18, x > 18 and x < 50, x > 50)\n",
    "discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
    "age_categories = discretize_layer(age)\n",
    "\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will create 3 bins automatically using percentiles\n",
    "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "age_categories = discretize_layer(age)\n",
    "\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CategoryEncoding Layer\n",
    "\n",
    "This layer does one-hot encoding for categorical data. It's great if there are up to a couple dozens categories (otherwise, it gets\n",
    "sparse).\n",
    "\n",
    "For example, we do one hot encoding for the age bins from the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
    "onehot_layer(age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The StringLookup Layer\n",
    "\n",
    "One-hot encode string information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "str_lookup_layer.adapt(cities)\n",
    "\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular string to index encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(cities)\n",
    "\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.01377795, -0.0329439 ],\n",
       "       [ 0.02590921, -0.01035757],\n",
       "       [-0.01377795, -0.0329439 ]], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(ocean_prox)\n",
    "\n",
    "# Returns an embedding of textual 1D data\n",
    "lookup_and_embed = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string),\n",
    "  str_lookup_layer,\n",
    "  # One row per category (input_dim) and one column per embedding dimension (output_dim)\n",
    "  tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(), output_dim=2)\n",
    "])\n",
    "\n",
    "lookup_and_embed(np.array([\"<1H OCEAN\", \"ISLAND\", \"<1H OCEAN\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for a model that can process categorical text along with regular numerical features to learn\n",
    "an embedding for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1839 - val_loss: 0.0982\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 0s 839us/step - loss: 0.0930 - val_loss: 0.0881\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 0s 841us/step - loss: 0.0865 - val_loss: 0.0845\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 0s 837us/step - loss: 0.0841 - val_loss: 0.0832\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 0s 846us/step - loss: 0.0833 - val_loss: 0.0828\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generating a fake dataset which has 8 numerical features (10K rows of random data in 8 dimensions)\n",
    "X_train_num = np.random.rand(10_000, 8)\n",
    "\n",
    "# Using the ocean_proximity array as the single categorical feature of this fake dataset\n",
    "X_train_cat = np.random.choice(ocean_prox, size=10_000)\n",
    "\n",
    "# Random label column\n",
    "y_train = np.random.rand(10_000, 1)\n",
    "\n",
    "# Repeating same steps to create a fake validation dataset\n",
    "X_valid_num = np.random.rand(2_000, 8)\n",
    "X_valid_cat = np.random.choice(ocean_prox, size=2_000)\n",
    "y_valid = np.random.rand(2_000, 1)\n",
    "\n",
    "# This model will take the two inputs separably\n",
    "num_inputs = tf.keras.layers.Input(shape=[8], name=\"num\")\n",
    "cat_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\n",
    "\n",
    "# This generates a random embedding for each category\n",
    "cat_embedding = lookup_and_embed(cat_inputs)\n",
    "\n",
    "# Concatenate the numerical and categorical input layers into a single input layer\n",
    "encoded_inputs = tf.keras.layers.concatenate([num_inputs, cat_embedding])\n",
    "\n",
    "# output layer - connected to input layer\n",
    "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "# Put the layers together in a model\n",
    "model = tf.keras.models.Model(\n",
    "  inputs=[num_inputs, cat_inputs], outputs=[outputs]\n",
    ")\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "\n",
    "history = model.fit((X_train_num, X_train_cat), y_train, epochs=5, validation_data=((X_valid_num, X_valid_cat), y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.49439093],\n",
       "       [0.47031915],\n",
       "       [0.48732972]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 new examples\n",
    "X_new_num = np.random.rand(3, 8)\n",
    "X_new_cat = np.array([\"<1H OCEAN\", \"INLAND\", \"ISLAND\"])\n",
    "\n",
    "model.predict((X_new_num, X_new_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "During `adapt()` it works by cleaning up the text (removing punctuation, lower casing, etc...). Then it builds a vocabulary where each word in it maps to an integer.\n",
    "\n",
    "When it receives data after adapt, it maps each word in it to the integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
       "array([[2, 1, 0, 0],\n",
       "       [6, 2, 1, 2]])>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "array([[0.96725637, 0.6931472 , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.96725637, 1.3862944 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does a similar adaptation as above only using the TF-IDF approach\n",
    "\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Hub\n",
    "\n",
    "Pre-trained models at the ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25,  0.28,  0.01,  0.1 ,  0.14,  0.16,  0.25,  0.02,  0.07,\n",
       "         0.13, -0.19,  0.06, -0.04, -0.07,  0.  , -0.08, -0.14, -0.16,\n",
       "         0.02, -0.24,  0.16, -0.16, -0.03,  0.03, -0.14,  0.03, -0.09,\n",
       "        -0.04, -0.14, -0.19,  0.07,  0.15,  0.18, -0.23, -0.07, -0.08,\n",
       "         0.01, -0.01,  0.09,  0.14, -0.03,  0.03,  0.08,  0.1 , -0.01,\n",
       "        -0.03, -0.07, -0.1 ,  0.05,  0.31],\n",
       "       [-0.2 ,  0.2 , -0.08,  0.02,  0.19,  0.05,  0.22, -0.09,  0.02,\n",
       "         0.19, -0.02, -0.14, -0.2 , -0.04,  0.01, -0.07, -0.22, -0.1 ,\n",
       "         0.16, -0.44,  0.31, -0.1 ,  0.23,  0.15, -0.05,  0.15, -0.13,\n",
       "        -0.04, -0.08, -0.16, -0.1 ,  0.13,  0.13, -0.18, -0.04,  0.03,\n",
       "        -0.1 , -0.07,  0.07,  0.03, -0.08,  0.02,  0.05,  0.07, -0.14,\n",
       "        -0.1 , -0.18, -0.13, -0.04,  0.15]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A pretrained model for 50 dimensional sentence embeddings\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
    "sentence_embeddings.numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(\"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 2s 998us/step - loss: 10.1879 - accuracy: 0.8325 - val_loss: 6.2497 - val_accuracy: 0.8733\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 1s 762us/step - loss: 5.6137 - accuracy: 0.8784 - val_loss: 5.3430 - val_accuracy: 0.8800\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 1s 764us/step - loss: 5.0128 - accuracy: 0.8839 - val_loss: 5.8197 - val_accuracy: 0.8797\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 1s 772us/step - loss: 4.8059 - accuracy: 0.8854 - val_loss: 5.5299 - val_accuracy: 0.8798\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 1s 763us/step - loss: 4.6596 - accuracy: 0.8871 - val_loss: 5.9560 - val_accuracy: 0.8712\n",
      "313/313 [==============================] - 0s 783us/step - loss: 5.4611 - accuracy: 0.8815\n"
     ]
    }
   ],
   "source": [
    "# Load and split in one go\n",
    "train_set, valid_set, test_set = tfds.load(\n",
    "    name=\"mnist\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "# shuffle\n",
    "train_set = train_set.shuffle(10_000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = valid_set.batch(32).cache()\n",
    "test_set = test_set.batch(32).cache()\n",
    "\n",
    "# build and train a model\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
