{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "input_dates = df['date_from']\n",
    "output_dates = df['date_to']\n",
    "\n",
    "# shuffle\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dataset_size = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example encoded input:  [15, 20, 29, 35, 20, 32, 37, 0, 3, 11, 1, 0, 5, 11, 9, 3]\n",
      "Example encoded output:  [3, 9, 7, 1, 10, 0, 1, 10, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Helper functions and utilities\n",
    "\n",
    "MONTH_MAP = {\n",
    "  \"January\": \"01\",\n",
    "  \"February\": \"02\",\n",
    "  \"March\": \"03\",\n",
    "  \"April\": \"04\",\n",
    "  \"May\": \"05\",\n",
    "  \"June\": \"06\",\n",
    "  \"July\": \"07\",\n",
    "  \"August\": \"08\",\n",
    "  \"September\": \"09\",\n",
    "  \"October\": \"10\",\n",
    "  \"November\": \"11\",\n",
    "  \"December\": \"12\",\n",
    "}\n",
    "MONTHS = MONTH_MAP.keys()\n",
    "\n",
    "# All possible input chars\n",
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "\n",
    "# All possible output chars\n",
    "OUTPUT_CHARS = \"0123456789-\"\n",
    "\n",
    "# Convert a date into char IDs\n",
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]\n",
    "\n",
    "# Covert char ids into a date\n",
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence]) for sequence in ids]\n",
    "\n",
    "print (\"Example encoded input: \", date_str_to_ids(input_dates[0], INPUT_CHARS))\n",
    "print (\"Example encoded output: \", date_str_to_ids(output_dates[0], OUTPUT_CHARS))\n",
    "\n",
    "# Convert all date strings into char id tensors\n",
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
    "\n",
    "# Prepare entire dataset\n",
    "def create_dataset(input_dates, output_dates):\n",
    "    return prepare_date_strs(input_dates, INPUT_CHARS), prepare_date_strs(output_dates, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and valid\n",
    "\n",
    "dataset = create_dataset(input_dates, output_dates)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * len(dataset[0]))\n",
    "\n",
    "X_train, y_train = dataset[0][:train_size], dataset[1][:train_size]\n",
    "X_valid, y_valid = dataset[0][train_size:], dataset[1][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 12s 25ms/step - loss: 1.7913 - accuracy: 0.3749 - val_loss: 1.6601 - val_accuracy: 0.3798\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 1.5568 - accuracy: 0.4491 - val_loss: 1.5725 - val_accuracy: 0.4098\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 5s 21ms/step - loss: 1.1869 - accuracy: 0.5722 - val_loss: 1.4895 - val_accuracy: 0.4896\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 1.1938 - accuracy: 0.5887 - val_loss: 1.7262 - val_accuracy: 0.3898\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 1.1364 - accuracy: 0.5998 - val_loss: 1.3078 - val_accuracy: 0.5274\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 5s 21ms/step - loss: 0.8273 - accuracy: 0.7005 - val_loss: 1.1231 - val_accuracy: 0.5824\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 5s 21ms/step - loss: 0.6502 - accuracy: 0.7580 - val_loss: 1.1094 - val_accuracy: 0.5875\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 5s 21ms/step - loss: 0.4915 - accuracy: 0.8129 - val_loss: 1.1976 - val_accuracy: 0.6094\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 0.3589 - accuracy: 0.8646 - val_loss: 0.9915 - val_accuracy: 0.7075\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.2623 - accuracy: 0.9066 - val_loss: 1.0100 - val_accuracy: 0.7401\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 0.1787 - accuracy: 0.9472 - val_loss: 0.7400 - val_accuracy: 0.8167\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 5s 22ms/step - loss: 0.1138 - accuracy: 0.9733 - val_loss: 0.7557 - val_accuracy: 0.8371\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 6s 23ms/step - loss: 0.0685 - accuracy: 0.9877 - val_loss: 0.6888 - val_accuracy: 0.8536\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 6s 25ms/step - loss: 0.4702 - accuracy: 0.8631 - val_loss: 0.8933 - val_accuracy: 0.7143\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.2575 - accuracy: 0.9347 - val_loss: 0.4859 - val_accuracy: 0.8312\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 6s 24ms/step - loss: 0.0886 - accuracy: 0.9891 - val_loss: 0.4970 - val_accuracy: 0.8382\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 5s 22ms/step - loss: 0.0474 - accuracy: 0.9962 - val_loss: 0.5387 - val_accuracy: 0.8339\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 6s 24ms/step - loss: 0.0306 - accuracy: 0.9983 - val_loss: 0.5620 - val_accuracy: 0.8359\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 6s 25ms/step - loss: 0.0215 - accuracy: 0.9990 - val_loss: 0.5776 - val_accuracy: 0.8363\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 6s 22ms/step - loss: 0.0157 - accuracy: 0.9993 - val_loss: 0.6023 - val_accuracy: 0.8376\n"
     ]
    }
   ],
   "source": [
    "# Encoder-decoder version 1\n",
    "\n",
    "embedding_size = 32\n",
    "max_output_length = y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=embedding_size, input_shape=[None]),\n",
    "    tf.keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['8888800874']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not sure what's the problem with this model. I did it almost exactly the same as the one in the book. \n",
    "# The only thing that may cause an issue is the data itself and its distribution.\n",
    "\n",
    "test_date = \"January 17, 1994\"\n",
    "test_date_encoded = date_str_to_ids(test_date, INPUT_CHARS)\n",
    "char_ids_inference = model.predict([test_date_encoded]).argmax(axis=-1)\n",
    "ids_to_date_strs(char_ids_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
