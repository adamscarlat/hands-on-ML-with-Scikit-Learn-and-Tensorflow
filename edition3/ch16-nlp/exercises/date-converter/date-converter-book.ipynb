{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "# Generate random dates\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "\n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "\n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "OUTPUT_CHARS = \"0123456789-\"\n",
    "\n",
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]\n",
    "\n",
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence]) for sequence in ids]\n",
    "\n",
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 8s 18ms/step - loss: 1.6968 - accuracy: 0.3943 - val_loss: 1.2574 - val_accuracy: 0.5494\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 1.2464 - accuracy: 0.5571 - val_loss: 0.9858 - val_accuracy: 0.6399\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.9658 - accuracy: 0.6502 - val_loss: 0.8167 - val_accuracy: 0.6928\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.7115 - accuracy: 0.7278 - val_loss: 0.6518 - val_accuracy: 0.7391\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.5578 - accuracy: 0.7796 - val_loss: 0.4684 - val_accuracy: 0.8116\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.6207 - accuracy: 0.7728 - val_loss: 0.8151 - val_accuracy: 0.7132\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.4086 - accuracy: 0.8500 - val_loss: 0.2917 - val_accuracy: 0.8917\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2319 - accuracy: 0.9210 - val_loss: 0.1926 - val_accuracy: 0.9401\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.1432 - accuracy: 0.9613 - val_loss: 0.1078 - val_accuracy: 0.9746\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1717 - accuracy: 0.9586 - val_loss: 0.0973 - val_accuracy: 0.9795\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.0649 - accuracy: 0.9884 - val_loss: 0.0495 - val_accuracy: 0.9920\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 7s 24ms/step - loss: 0.0370 - accuracy: 0.9957 - val_loss: 0.0318 - val_accuracy: 0.9958\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0232 - accuracy: 0.9982 - val_loss: 0.0223 - val_accuracy: 0.9973\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0152 - accuracy: 0.9995 - val_loss: 0.0146 - val_accuracy: 0.9991\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.0103 - accuracy: 0.9998 - val_loss: 0.0105 - val_accuracy: 0.9993\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0071 - accuracy: 0.9999 - val_loss: 0.0074 - val_accuracy: 0.9997\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 0.9998\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 0.9999\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = tf.keras.Sequential([\n",
    "    # map each character to an embedded vector of size 32\n",
    "    tf.keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=embedding_size, input_shape=[None]),\n",
    "    tf.keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    # the decoder expects a sequence, we simply repeat the encoder's output \n",
    "    tf.keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 424ms/step\n",
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])\n",
    "ids = model.predict(X_new).argmax(axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 - teacher forcing encoder-decoder\n",
    "\n",
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 8s 19ms/step - loss: 1.6375 - accuracy: 0.3944 - val_loss: 1.3159 - val_accuracy: 0.5192\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 1.0655 - accuracy: 0.6208 - val_loss: 0.8243 - val_accuracy: 0.7136\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6308 - accuracy: 0.7791 - val_loss: 0.4539 - val_accuracy: 0.8440\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2962 - accuracy: 0.9120 - val_loss: 0.1996 - val_accuracy: 0.9470\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.1087 - accuracy: 0.9824 - val_loss: 0.0694 - val_accuracy: 0.9936\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0720 - accuracy: 0.9898 - val_loss: 0.0495 - val_accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 7s 24ms/step - loss: 0.0310 - accuracy: 0.9986 - val_loss: 0.0230 - val_accuracy: 0.9995\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.0179 - accuracy: 0.9997 - val_loss: 0.0155 - val_accuracy: 0.9998\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.0437 - accuracy: 0.9928 - val_loss: 0.0138 - val_accuracy: 0.9997\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.0102 - accuracy: 0.9999 - val_loss: 0.0084 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = tf.keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = tf.keras.layers.LSTM(lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = tf.keras.layers.Embedding(input_dim=len(OUTPUT_CHARS) + 2, output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = tf.keras.layers.LSTM(lstm_units, return_sequences=True)(decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10, validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    ids = model.predict(X).argmax(axis=-1)\n",
    "    return ids_to_date_strs(ids)\n",
    "\n",
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "        \n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14', '1778-08-22']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"May 02, 2020\", \"July 14, 1789\", \"Augussst 22, 1778\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
