{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers\n",
    "-------\n",
    "1. Stateful RNNs use the hidden state between operations. The final output of an RNN cell in a stateful RNN is used\n",
    "  as input in the next sequence. Maintaining state between iterations can help building a better long term memory, as\n",
    "  information is shared between sequences.\n",
    "  This statefulness is only beneficial when there is dependency between the sequences in the dataset. For example, a time \n",
    "  series that predicts power consumption. Usually these datasets are split in sequences of 14 steps for learning. \n",
    "  Datasets in which the sequences are independent of each other will not benefit from stateful RNN, in fact, it can be\n",
    "  bad for learning in such datasets. For example, sentiment analysis based on reviews.\n",
    "\n",
    "2. When training a model for seq-to-seq, using encoder-decoders allows decoupling of the encoder model from the \n",
    "  decoder model. This approach means that we can use different architectures in each part. For example, having output sequences of different lengths  than the input sequences. This opens the door to tasks such as neural machine translation, in which the input sequence is in one language and the output sequence is in another language and their length might not match. In addition, \n",
    "  the encoder-decoder allows for the encoder to read the entire sentence before the translation starts, which is necessary\n",
    "  when translating a sentence.\n",
    "\n",
    "3. RNNs (and their derivatives - LSTM and GRUs) allow for variable input sequence lengths. This is because of their recursive\n",
    "  nature. each element in the sequence is inputted into the same RNN cell along with any previous hidden state (previous outputs).\n",
    "  The result of the cell can be a vector - the final predicted timestep - or a sequence - each timestep prediction from the \n",
    "  unrolled RNN cell.\n",
    "  If the sequences from an RNN layer are passed to a different type of layer such as Dense, they all must be in the same length.\n",
    "  This can be achieved using techniques such as setting a max sequence length and then use padding and cropping on all the sequences\n",
    "  so that they match.\n",
    "  The output sequence of an RNN cell can be equal or less than the input sequence (simply by taking less output step results).\n",
    "  If the output sequence needs to be independent (in length) than the input sequence, it's possible to use an encoder-decoder model.\n",
    "\n",
    "4. Beam search is a technique that improves the performance of RNN based seq-to-seq models. When we have a model that increases\n",
    "  its prompt and re-generates a next token (over and over), we can take the k next probable tokens at each step and build k \n",
    "  prompts simultaneously. After the k prompts are complete, we compute their conditional probability by multiplying each one\n",
    "  of the k probabilities of each inference in the prompt. This can help in situations where a single, high probability but \n",
    "  incorrect token swayed the next tokens and made the entire result sequence incorrect. \n",
    "\n",
    "5. The attention mechanism is a technique to enhance performance of seq-to-seq models and encoders decoders. The main idea behind\n",
    "  attention is to add the concept of context into an embedded vector by taking a function between each word in the sequence to each\n",
    "  other word in the sequence. Attention helps maintain longer term memory to tokens in the first part of the sequence for longer \n",
    "  sequences.\n",
    "  For example, in an encoder-decoder model used for machine translation, we add attention between the encoder outputs (at each\n",
    "  time step) and the decoder outputs. \n",
    "\n",
    " 6. The most important layer in a transformer is the self-attention layer. This layer introduces context to each one of the\n",
    "  embedded vectors. It does that by learning which words are most aligned with which other words between the encoder and the\n",
    "  decoder.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
