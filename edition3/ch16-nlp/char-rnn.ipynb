{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text Using Character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Shakespeare text\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "  shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "# Printing the first few lines\n",
    "\n",
    "print (shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  1115394\n",
      "Number of unique characters:  39\n"
     ]
    }
   ],
   "source": [
    "# Processing the text \n",
    "\n",
    "# Vectorizing by character - each character is now mapped to an integer\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded_text = text_vec_layer([shakespeare_text])[0]\n",
    "\n",
    "# The TextVectorization layer uses 0 for padding and 1 for unknown chars. We don't need them\n",
    "# in this case, so we can deduct 2 from all character keys so that they start at 0\n",
    "encoded_text -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded_text)\n",
    "\n",
    "print (\"Total number of characters: \", dataset_size)\n",
    "print (\"Number of unique characters: \", n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set\n",
    "\n",
    "# We're going to create windows from the text. For example, \n",
    "#   A training example can be - \"to be or not to b\"\n",
    "#   And it's corresponding label - \"o be or not to be\"\n",
    "\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "  # Convert the tensor into a Dataset\n",
    "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "\n",
    "  # Generate windows of length - length+1. Drop last windows that are less than desired size\n",
    "  ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "  # Map windows to 1D arrays (using the batch method)\n",
    "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "  \n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "\n",
    "  ds = ds.batch(batch_size)\n",
    "  \n",
    "  # Map the arrays to training and label (see example above).\n",
    "  # Set the Prefetch so that the next batch load can start while the current batch is used in training\n",
    "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / valid / test split\n",
    "\n",
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = to_dataset(encoded_text[:1_000_000], length=length, shuffle=True)\n",
    "valid_set = to_dataset(encoded_text[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded_text[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "  tf.keras.layers.GRU(128, return_sequences=True),\n",
    "  tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on kaggle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the trained model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39mmodels/my_shakespeare_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Wrap it with the preprocessing step\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m shakespeare_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   text_vec_layer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLambda(\u001b[39mlambda\u001b[39;00m X: X \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m), \u001b[39m# no PAD or UNKNOWN tokens\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   loaded_model\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "loaded_model = tf.keras.models.load_model(\"models/my_shakespeare_model\")\n",
    "\n",
    "# Wrap it with the preprocessing step\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "  text_vec_layer,\n",
    "  tf.keras.layers.Lambda(lambda X: X - 2), # no PAD or UNKNOWN tokens\n",
    "  loaded_model\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shakespeare_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m next_char(text_model, text, temperature)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m text\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adamscarlat/Documents/Github/hands-on-ML-with-Scikit-Learn-and-Tensorflow/edition3/ch16-nlp/char-rnn.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m (extend_text(shakespeare_model, \u001b[39m\"\u001b[39m\u001b[39mTo be or not to be\u001b[39m\u001b[39m\"\u001b[39m, n_chars\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shakespeare_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Generating text\n",
    "\n",
    "# Since this model outputs one character at a time, we can add the predicted character to the \n",
    "# seed text and resend to the model for prediction in a loop. This approach is called \"greedy decoding\" and\n",
    "# in practice it just repeats the same word over and over.\n",
    "\n",
    "# Instead we'll output all the probabilities of the next character and choose the next one according to a \n",
    "# parameter called \"temperature\". This parameter is between 0-1. Values closer to 0 will choose the higher\n",
    "# probability character whereas values closer to 1 will choose the lower probability ones, adding to the\n",
    "# randomness.\n",
    "\n",
    "def next_char(text_model, text, temperature=1):\n",
    "  y_proba = text_model.predict([text])[0, -1:]\n",
    "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "  char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "\n",
    "  return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "def extend_text(text_model, text, n_chars=50, temperature=1):\n",
    "  for _ in range(n_chars):\n",
    "    text += next_char(text_model, text, temperature)\n",
    "  \n",
    "  return text\n",
    "\n",
    "print (extend_text(shakespeare_model, \"To be or not to be\", n_chars=100, temperature=0.3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
