{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on the IMDB Reviews Dataset\n",
    "\n",
    "An example of a word based RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 08:37:51.446449: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/adamscarlat/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 80/80 [00:20<00:00,  3.88 MiB/s]rl]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:20<00:00, 20.62s/ url]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/adamscarlat/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "  name=\"imdb_reviews\",\n",
    "  split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "  as_supervised=True\n",
    ")\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"This was an absolutely terrible movie. Don't be lured in by Christopher \"\n",
      " 'Walken or Michael Ironside. Both are great actors, but this must simply be '\n",
      " 'their worst role in history. Even their great acting could not redeem this '\n",
      " \"movie's ridiculous storyline. This movie is an early nineties US propaganda \"\n",
      " 'piece. The most pathetic scenes were those when the Columbian rebels were '\n",
      " 'making their cases for revolutions. Maria Conchita Alonso appeared phony, '\n",
      " 'and her pseudo-love affair with Walken was nothing but a pathetic emotional '\n",
      " 'plug in a movie that was devoid of any real meaning. I am disappointed that '\n",
      " \"there are movies like this, ruining actor's like Christopher Walken's good \"\n",
      " 'name. I could barely sit through it.')\n",
      "Label:  0\n",
      "('I have been known to fall asleep during films, but this is usually due to a '\n",
      " 'combination of things including, really tired, being warm and comfortable on '\n",
      " 'the sette and having just eaten a lot. However on this occasion I fell '\n",
      " 'asleep because the film was rubbish. The plot development was constant. '\n",
      " 'Constantly slow and boring. Things seemed to happen, but with no explanation '\n",
      " 'of what was causing them or why. I admit, I may have missed part of the '\n",
      " 'film, but i watched the majority of it and everything just seemed to happen '\n",
      " 'of its own accord without any real concern for anything else. I cant '\n",
      " 'recommend this film at all.')\n",
      "Label:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 08:47:35.332253: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the data\n",
    "\n",
    "for review, label in raw_train_set.take(2):\n",
    "  pprint (review.numpy().decode(\"utf-8\"))\n",
    "  print (\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the text into words\n",
    "\n",
    "vocab_size = 1000\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "\n",
    "# We only want to adapt the vectorizer on the reviews (not the labels)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))\n",
    "\n",
    "embed_size = 128\n",
    "model = tf.keras.Sequential([\n",
    "  text_vec_layer,\n",
    "  # This embedding matrix has a row per token and 'embed_size' columns. This way each\n",
    "  # token gets mapped to a vector of size 'embed_size'.\n",
    "  tf.keras.layers.Embedding(vocab_size, embed_size),\n",
    "  tf.keras.layers.GRU(128),\n",
    "  # 1 neuron for classifying positive/negative sentiments\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Training this model (on Kaggle) yields disappointing results. The accuracy does not increase above 50%. The main reason\n",
    "for it is that the reviews are of different lengths and the TextVectorization layer uses the longest sequence as a \n",
    "limit and pads other sequences with zeros to match the longest sequence's length. This is causing the GRU to learn bad \n",
    "patterns as the padding affect the learning a lot.\n",
    "\n",
    "One way to deal with it is using `masking` - making the model ignore padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the text into words\n",
    "\n",
    "# The only difference between this setup and the previous one is the `mask_zero=True` parameter to the embedding layer!\n",
    "# Training this model on Kaggle yields results with validation accuracy over 85%.\n",
    "\n",
    "vocab_size = 1000\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "\n",
    "# We only want to adapt the vectorizer on the reviews (not the labels)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))\n",
    "\n",
    "embed_size = 128\n",
    "model = tf.keras.Sequential([\n",
    "  text_vec_layer,\n",
    "  tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
    "  tf.keras.layers.GRU(128),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\"imdb_sentiment_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[ model_ckpt ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on Kaggle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 08:19:10.786856: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:10.888661: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:10.900962: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:10.987421: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:11.033969: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:11.046972: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:11.652082: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:11.766976: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:11.779251: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:11.798858: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:12.380760: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:12.501019: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:12.513561: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-11-08 08:19:12.535175: W tensorflow/core/common_runtime/graph_constructor.cc:839] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"models/imdb_sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.03552284]]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[0.91320604]]\n"
     ]
    }
   ],
   "source": [
    "print (loaded_model.predict([\"bro that movie sucked real bad. stroy line was awful\"]))\n",
    "print (loaded_model.predict([\"bro that movie was amazing. stroy line was superb\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained LLM Parts in Our Models\n",
    "\n",
    "Instead of training an embedding layer from scratch or reusing a pretrained embedded layer, we reuse a part of a pretrained language\n",
    "  model.\n",
    "  - These pretrained LLM parts take into account context and embeddings.\n",
    "  - We can further fine-tune them when we add them to our models.\n",
    "\n",
    "After training this model, we reach an accuracy of over 90% since context is taken into account. This helps with reviews such as \n",
    "\"this movie was not as great as I hoped\" - notice that the presence of \"great\" is actually negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid having the model download over and over \n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=True, dtype=tf.string, input_shape=[]),\n",
    "  tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\"models/imdb_sentiment_model_use\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[ model_ckpt ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"models/imdb_sentiment_model_use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "[[0.08436868]]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[0.73675543]]\n"
     ]
    }
   ],
   "source": [
    "# Doesn't seem to catch the inverse statements from context (e.g. \"not good\", \"not great\") as advertised \n",
    "\n",
    "print (loaded_model.predict([\"bro this movie is not bad. I actually thought that it was not as boring as other ones\"]))\n",
    "print (loaded_model.predict([\"this movie was not as good as I hoped. I thought that it was not an interesting movie\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
